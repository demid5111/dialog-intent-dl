{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PredictOnColab.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/demid5111/dialog-intent-dl/blob/master/ipynb/predict_intent_with_embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tE1UJlzMur_k",
        "colab_type": "code",
        "outputId": "645d1bc1-5e03-4abf-c6c1-2082732e7220",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anjRwJ1o0yqo",
        "colab_type": "code",
        "outputId": "e83b78ad-2608-400b-a329-063619f9e7c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "! tar -xzf \"/content/drive/My Drive/Projects/RFFI2016/dataset.tar.gz\" -C /content\n",
        "! ls \"/content/feature_and_vector_seq\" | wc -l"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "13156\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWDAX-g82iqa",
        "colab_type": "code",
        "outputId": "987cc819-86cb-46e0-e7e0-af282bfe0cd2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "! git clone https://github.com/demid5111/dialog-intent-dl.git\n",
        "! mv dialog-intent-dl dialog_intent_dl\n",
        "! pip install -r dialog_intent_dl/requirements.txt\n",
        "! pip install tensorflow-gpu==1.13.1\n",
        "! pip install numpy==1.15.4\n",
        "# restart runtime\n",
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'dialog-intent-dl'...\n",
            "remote: Enumerating objects: 37, done.\u001b[K\n",
            "remote: Counting objects:   2% (1/37)\u001b[K\rremote: Counting objects:   5% (2/37)\u001b[K\rremote: Counting objects:   8% (3/37)\u001b[K\rremote: Counting objects:  10% (4/37)\u001b[K\rremote: Counting objects:  13% (5/37)\u001b[K\rremote: Counting objects:  16% (6/37)\u001b[K\rremote: Counting objects:  18% (7/37)\u001b[K\rremote: Counting objects:  21% (8/37)\u001b[K\rremote: Counting objects:  24% (9/37)\u001b[K\rremote: Counting objects:  27% (10/37)\u001b[K\rremote: Counting objects:  29% (11/37)\u001b[K\rremote: Counting objects:  32% (12/37)\u001b[K\rremote: Counting objects:  35% (13/37)\u001b[K\rremote: Counting objects:  37% (14/37)\u001b[K\rremote: Counting objects:  40% (15/37)\u001b[K\rremote: Counting objects:  43% (16/37)\u001b[K\rremote: Counting objects:  45% (17/37)\u001b[K\rremote: Counting objects:  48% (18/37)\u001b[K\rremote: Counting objects:  51% (19/37)\u001b[K\rremote: Counting objects:  54% (20/37)\u001b[K\rremote: Counting objects:  56% (21/37)\u001b[K\rremote: Counting objects:  59% (22/37)\u001b[K\rremote: Counting objects:  62% (23/37)\u001b[K\rremote: Counting objects:  64% (24/37)\u001b[K\rremote: Counting objects:  67% (25/37)\u001b[K\rremote: Counting objects:  70% (26/37)\u001b[K\rremote: Counting objects:  72% (27/37)\u001b[K\rremote: Counting objects:  75% (28/37)\u001b[K\rremote: Counting objects:  78% (29/37)\u001b[K\rremote: Counting objects:  81% (30/37)\u001b[K\rremote: Counting objects:  83% (31/37)\u001b[K\rremote: Counting objects:  86% (32/37)\u001b[K\rremote: Counting objects:  89% (33/37)\u001b[K\rremote: Counting objects:  91% (34/37)\u001b[K\rremote: Counting objects:  94% (35/37)\u001b[K\rremote: Counting objects:  97% (36/37)\u001b[K\rremote: Counting objects: 100% (37/37)\u001b[K\rremote: Counting objects: 100% (37/37), done.\u001b[K\n",
            "remote: Compressing objects:   3% (1/28)\u001b[K\rremote: Compressing objects:   7% (2/28)\u001b[K\rremote: Compressing objects:  10% (3/28)\u001b[K\rremote: Compressing objects:  14% (4/28)\u001b[K\rremote: Compressing objects:  17% (5/28)\u001b[K\rremote: Compressing objects:  21% (6/28)\u001b[K\rremote: Compressing objects:  25% (7/28)\u001b[K\rremote: Compressing objects:  28% (8/28)\u001b[K\rremote: Compressing objects:  32% (9/28)\u001b[K\rremote: Compressing objects:  35% (10/28)\u001b[K\rremote: Compressing objects:  39% (11/28)\u001b[K\rremote: Compressing objects:  42% (12/28)\u001b[K\rremote: Compressing objects:  46% (13/28)\u001b[K\rremote: Compressing objects:  50% (14/28)\u001b[K\rremote: Compressing objects:  53% (15/28)\u001b[K\rremote: Compressing objects:  57% (16/28)\u001b[K\rremote: Compressing objects:  60% (17/28)\u001b[K\rremote: Compressing objects:  64% (18/28)\u001b[K\rremote: Compressing objects:  67% (19/28)\u001b[K\rremote: Compressing objects:  71% (20/28)\u001b[K\rremote: Compressing objects:  75% (21/28)\u001b[K\rremote: Compressing objects:  78% (22/28)\u001b[K\rremote: Compressing objects:  82% (23/28)\u001b[K\rremote: Compressing objects:  85% (24/28)\u001b[K\rremote: Compressing objects:  89% (25/28)\u001b[K\rremote: Compressing objects:  92% (26/28)\u001b[K\rremote: Compressing objects:  96% (27/28)\u001b[K\rremote: Compressing objects: 100% (28/28)\u001b[K\rremote: Compressing objects: 100% (28/28), done.\u001b[K\n",
            "Receiving objects:   0% (1/185)   \rReceiving objects:   1% (2/185)   \rReceiving objects:   2% (4/185)   \rReceiving objects:   3% (6/185)   \rReceiving objects:   4% (8/185)   \rReceiving objects:   5% (10/185)   \rReceiving objects:   6% (12/185)   \rReceiving objects:   7% (13/185)   \rReceiving objects:   8% (15/185)   \rReceiving objects:   9% (17/185)   \rReceiving objects:  10% (19/185)   \rReceiving objects:  11% (21/185)   \rReceiving objects:  12% (23/185)   \rReceiving objects:  13% (25/185)   \rReceiving objects:  14% (26/185)   \rReceiving objects:  15% (28/185)   \rReceiving objects:  16% (30/185)   \rReceiving objects:  17% (32/185)   \rReceiving objects:  18% (34/185)   \rReceiving objects:  19% (36/185)   \rReceiving objects:  20% (37/185)   \rReceiving objects:  21% (39/185)   \rReceiving objects:  22% (41/185)   \rReceiving objects:  23% (43/185)   \rReceiving objects:  24% (45/185)   \rReceiving objects:  25% (47/185)   \rReceiving objects:  26% (49/185)   \rReceiving objects:  27% (50/185)   \rReceiving objects:  28% (52/185)   \rReceiving objects:  29% (54/185)   \rReceiving objects:  30% (56/185)   \rReceiving objects:  31% (58/185)   \rReceiving objects:  32% (60/185)   \rReceiving objects:  33% (62/185)   \rReceiving objects:  34% (63/185)   \rReceiving objects:  35% (65/185)   \rReceiving objects:  36% (67/185)   \rReceiving objects:  37% (69/185)   \rReceiving objects:  38% (71/185)   \rReceiving objects:  39% (73/185)   \rReceiving objects:  40% (74/185)   \rReceiving objects:  41% (76/185)   \rReceiving objects:  42% (78/185)   \rReceiving objects:  43% (80/185)   \rReceiving objects:  44% (82/185)   \rReceiving objects:  45% (84/185)   \rReceiving objects:  46% (86/185)   \rReceiving objects:  47% (87/185)   \rReceiving objects:  48% (89/185)   \rReceiving objects:  49% (91/185)   \rReceiving objects:  50% (93/185)   \rReceiving objects:  51% (95/185)   \rReceiving objects:  52% (97/185)   \rReceiving objects:  53% (99/185)   \rReceiving objects:  54% (100/185)   \rReceiving objects:  55% (102/185)   \rReceiving objects:  56% (104/185)   \rReceiving objects:  57% (106/185)   \rReceiving objects:  58% (108/185)   \rReceiving objects:  59% (110/185)   \rReceiving objects:  60% (111/185)   \rReceiving objects:  61% (113/185)   \rReceiving objects:  62% (115/185)   \rReceiving objects:  63% (117/185)   \rReceiving objects:  64% (119/185)   \rReceiving objects:  65% (121/185)   \rReceiving objects:  66% (123/185)   \rReceiving objects:  67% (124/185)   \rReceiving objects:  68% (126/185)   \rReceiving objects:  69% (128/185)   \rReceiving objects:  70% (130/185)   \rReceiving objects:  71% (132/185)   \rReceiving objects:  72% (134/185)   \rReceiving objects:  73% (136/185)   \rReceiving objects:  74% (137/185)   \rReceiving objects:  75% (139/185)   \rReceiving objects:  76% (141/185)   \rReceiving objects:  77% (143/185)   \rReceiving objects:  78% (145/185)   \rReceiving objects:  79% (147/185)   \rReceiving objects:  80% (148/185)   \rReceiving objects:  81% (150/185)   \rReceiving objects:  82% (152/185)   \rReceiving objects:  83% (154/185)   \rReceiving objects:  84% (156/185)   \rReceiving objects:  85% (158/185)   \rReceiving objects:  86% (160/185)   \rReceiving objects:  87% (161/185)   \rReceiving objects:  88% (163/185)   \rReceiving objects:  89% (165/185)   \rReceiving objects:  90% (167/185)   \rremote: Total 185 (delta 19), reused 20 (delta 9), pack-reused 148\u001b[K\n",
            "Receiving objects:  91% (169/185)   \rReceiving objects:  92% (171/185)   \rReceiving objects:  93% (173/185)   \rReceiving objects:  94% (174/185)   \rReceiving objects:  95% (176/185)   \rReceiving objects:  96% (178/185)   \rReceiving objects:  97% (180/185)   \rReceiving objects:  98% (182/185)   \rReceiving objects:  99% (184/185)   \rReceiving objects: 100% (185/185)   \rReceiving objects: 100% (185/185), 178.81 KiB | 13.75 MiB/s, done.\n",
            "Resolving deltas: 100% (91/91), done.\n",
            "Collecting nltk==3.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/09/3b1755d528ad9156ee7243d52aa5cd2b809ef053a0f31b53d92853dd653a/nltk-3.3.0.zip (1.4MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4MB 5.1MB/s \n",
            "\u001b[?25hCollecting gevent==1.3.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ef/71/48aa308c1a92475e89afbd1a45d6978ac69b007169c761f048a5c9336f2d/gevent-1.3.4-cp36-cp36m-manylinux1_x86_64.whl (4.5MB)\n",
            "\u001b[K     |████████████████████████████████| 4.5MB 38.4MB/s \n",
            "\u001b[?25hCollecting gensim==3.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/33/33/df6cb7acdcec5677ed130f4800f67509d24dbec74a03c329fcbf6b0864f0/gensim-3.4.0-cp36-cp36m-manylinux1_x86_64.whl (22.6MB)\n",
            "\u001b[K     |████████████████████████████████| 22.6MB 55.9MB/s \n",
            "\u001b[?25hCollecting networkx==2.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/11/42/f951cc6838a4dff6ce57211c4d7f8444809ccbe2134179950301e5c4c83c/networkx-2.1.zip (1.6MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6MB 43.0MB/s \n",
            "\u001b[?25hCollecting numpy==1.15.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ff/7f/9d804d2348471c67a7d8b5f84f9bc59fd1cefa148986f2b74552f8573555/numpy-1.15.4-cp36-cp36m-manylinux1_x86_64.whl (13.9MB)\n",
            "\u001b[K     |████████████████████████████████| 13.9MB 36.9MB/s \n",
            "\u001b[?25hCollecting pandas==0.23.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/57/eb/6ab533ea8e35e7dd159af6922ac1123d4565d89f3926ad9a6aa46530978f/pandas-0.23.1-cp36-cp36m-manylinux1_x86_64.whl (11.8MB)\n",
            "\u001b[K     |████████████████████████████████| 11.8MB 19.9MB/s \n",
            "\u001b[?25hCollecting scipy==1.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/0b/f163da98d3a01b3e0ef1cab8dd2123c34aee2bafbb1c5bffa354cc8a1730/scipy-1.1.0-cp36-cp36m-manylinux1_x86_64.whl (31.2MB)\n",
            "\u001b[K     |████████████████████████████████| 31.2MB 25.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: xlrd==1.1.0 in /usr/local/lib/python3.6/dist-packages (from -r dialog_intent_dl/requirements.txt (line 8)) (1.1.0)\n",
            "Collecting xlsxwriter==1.0.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/33/50/136b801d106fcebb2428a764e5c599e020d8227a3623db078e05eb4793a5/XlsxWriter-1.0.5-py2.py3-none-any.whl (142kB)\n",
            "\u001b[K     |████████████████████████████████| 143kB 42.2MB/s \n",
            "\u001b[?25hCollecting tensorflow-gpu==1.13.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7b/b1/0ad4ae02e17ddd62109cd54c291e311c4b5fd09b4d0678d3d6ce4159b0f0/tensorflow_gpu-1.13.1-cp36-cp36m-manylinux1_x86_64.whl (345.2MB)\n",
            "\u001b[K     |████████████████████████████████| 345.2MB 49kB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk==3.3.0->-r dialog_intent_dl/requirements.txt (line 1)) (1.12.0)\n",
            "Requirement already satisfied: greenlet>=0.4.13; platform_python_implementation == \"CPython\" in /usr/local/lib/python3.6/dist-packages (from gevent==1.3.4->-r dialog_intent_dl/requirements.txt (line 2)) (0.4.15)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim==3.4.0->-r dialog_intent_dl/requirements.txt (line 3)) (1.8.4)\n",
            "Requirement already satisfied: decorator>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from networkx==2.1.0->-r dialog_intent_dl/requirements.txt (line 4)) (4.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python3.6/dist-packages (from pandas==0.23.1->-r dialog_intent_dl/requirements.txt (line 6)) (2.5.3)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas==0.23.1->-r dialog_intent_dl/requirements.txt (line 6)) (2018.9)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1->-r dialog_intent_dl/requirements.txt (line 10)) (3.10.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1->-r dialog_intent_dl/requirements.txt (line 10)) (1.1.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1->-r dialog_intent_dl/requirements.txt (line 10)) (1.15.0)\n",
            "Collecting tensorboard<1.14.0,>=1.13.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/39/bdd75b08a6fba41f098b6cb091b9e8c7a80e1b4d679a581a0ccd17b10373/tensorboard-1.13.1-py3-none-any.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 31.0MB/s \n",
            "\u001b[?25hCollecting tensorflow-estimator<1.14.0rc0,>=1.13.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/48/13f49fc3fa0fdf916aa1419013bb8f2ad09674c275b4046d5ee669a46873/tensorflow_estimator-1.13.0-py2.py3-none-any.whl (367kB)\n",
            "\u001b[K     |████████████████████████████████| 368kB 44.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1->-r dialog_intent_dl/requirements.txt (line 10)) (1.0.8)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1->-r dialog_intent_dl/requirements.txt (line 10)) (0.8.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1->-r dialog_intent_dl/requirements.txt (line 10)) (0.33.6)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1->-r dialog_intent_dl/requirements.txt (line 10)) (1.1.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1->-r dialog_intent_dl/requirements.txt (line 10)) (0.2.2)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1->-r dialog_intent_dl/requirements.txt (line 10)) (0.8.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim==3.4.0->-r dialog_intent_dl/requirements.txt (line 3)) (2.21.0)\n",
            "Requirement already satisfied: boto>=2.32 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim==3.4.0->-r dialog_intent_dl/requirements.txt (line 3)) (2.49.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim==3.4.0->-r dialog_intent_dl/requirements.txt (line 3)) (1.9.253)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu==1.13.1->-r dialog_intent_dl/requirements.txt (line 10)) (41.4.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow-gpu==1.13.1->-r dialog_intent_dl/requirements.txt (line 10)) (3.1.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow-gpu==1.13.1->-r dialog_intent_dl/requirements.txt (line 10)) (0.16.0)\n",
            "Collecting mock>=2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/05/d2/f94e68be6b17f46d2c353564da56e6fb89ef09faeeff3313a046cb810ca9/mock-3.0.5-py2.py3-none-any.whl\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu==1.13.1->-r dialog_intent_dl/requirements.txt (line 10)) (2.8.0)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim==3.4.0->-r dialog_intent_dl/requirements.txt (line 3)) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim==3.4.0->-r dialog_intent_dl/requirements.txt (line 3)) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim==3.4.0->-r dialog_intent_dl/requirements.txt (line 3)) (2019.9.11)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim==3.4.0->-r dialog_intent_dl/requirements.txt (line 3)) (3.0.4)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim==3.4.0->-r dialog_intent_dl/requirements.txt (line 3)) (0.2.1)\n",
            "Requirement already satisfied: botocore<1.13.0,>=1.12.253 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim==3.4.0->-r dialog_intent_dl/requirements.txt (line 3)) (1.12.253)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim==3.4.0->-r dialog_intent_dl/requirements.txt (line 3)) (0.9.4)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.253->boto3->smart-open>=1.2.1->gensim==3.4.0->-r dialog_intent_dl/requirements.txt (line 3)) (0.15.2)\n",
            "Building wheels for collected packages: nltk, networkx\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.3-cp36-none-any.whl size=1394469 sha256=9bf543875661d567b7e9d230120662d8834121e414f708a6e762e956ee88009f\n",
            "  Stored in directory: /root/.cache/pip/wheels/d1/ab/40/3bceea46922767e42986aef7606a600538ca80de6062dc266c\n",
            "  Building wheel for networkx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for networkx: filename=networkx-2.1-py2.py3-none-any.whl size=1447766 sha256=035d87152b059bee830d4f9ea11286eda96e16f964e9a28ca74a39a060c8090d\n",
            "  Stored in directory: /root/.cache/pip/wheels/44/c0/34/6f98693a554301bdb405f8d65d95bbcd3e50180cbfdd98a94e\n",
            "Successfully built nltk networkx\n",
            "\u001b[31mERROR: tensorflow 1.15.0 has requirement numpy<2.0,>=1.16.0, but you'll have numpy 1.15.4 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 1.15.0 has requirement tensorboard<1.16.0,>=1.15.0, but you'll have tensorboard 1.13.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 1.15.0 has requirement tensorflow-estimator==1.15.1, but you'll have tensorflow-estimator 1.13.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: plotnine 0.5.1 has requirement pandas>=0.23.4, but you'll have pandas 0.23.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: mizani 0.5.4 has requirement pandas>=0.23.4, but you'll have pandas 0.23.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement pandas~=0.24.0, but you'll have pandas 0.23.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: fbprophet 0.5 has requirement pandas>=0.23.4, but you'll have pandas 0.23.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: nltk, gevent, numpy, scipy, gensim, networkx, pandas, xlsxwriter, tensorboard, mock, tensorflow-estimator, tensorflow-gpu\n",
            "  Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "  Found existing installation: gevent 1.4.0\n",
            "    Uninstalling gevent-1.4.0:\n",
            "      Successfully uninstalled gevent-1.4.0\n",
            "  Found existing installation: numpy 1.16.5\n",
            "    Uninstalling numpy-1.16.5:\n",
            "      Successfully uninstalled numpy-1.16.5\n",
            "  Found existing installation: scipy 1.3.1\n",
            "    Uninstalling scipy-1.3.1:\n",
            "      Successfully uninstalled scipy-1.3.1\n",
            "  Found existing installation: gensim 3.6.0\n",
            "    Uninstalling gensim-3.6.0:\n",
            "      Successfully uninstalled gensim-3.6.0\n",
            "  Found existing installation: networkx 2.4\n",
            "    Uninstalling networkx-2.4:\n",
            "      Successfully uninstalled networkx-2.4\n",
            "  Found existing installation: pandas 0.24.2\n",
            "    Uninstalling pandas-0.24.2:\n",
            "      Successfully uninstalled pandas-0.24.2\n",
            "  Found existing installation: tensorboard 1.15.0\n",
            "    Uninstalling tensorboard-1.15.0:\n",
            "      Successfully uninstalled tensorboard-1.15.0\n",
            "  Found existing installation: tensorflow-estimator 1.15.1\n",
            "    Uninstalling tensorflow-estimator-1.15.1:\n",
            "      Successfully uninstalled tensorflow-estimator-1.15.1\n",
            "Successfully installed gensim-3.4.0 gevent-1.3.4 mock-3.0.5 networkx-2.1 nltk-3.3 numpy-1.15.4 pandas-0.23.1 scipy-1.1.0 tensorboard-1.13.1 tensorflow-estimator-1.13.0 tensorflow-gpu-1.13.1 xlsxwriter-1.0.5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy",
                  "pandas"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow-gpu==1.13.1 in /usr/local/lib/python3.6/dist-packages (1.13.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (1.15.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (1.1.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (1.1.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (3.10.0)\n",
            "Requirement already satisfied: tensorboard<1.14.0,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (1.13.1)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (0.8.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (1.12.0)\n",
            "Requirement already satisfied: tensorflow-estimator<1.14.0rc0,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (1.13.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (0.8.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (1.15.4)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (0.2.2)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (0.33.6)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (1.0.8)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu==1.13.1) (41.4.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow-gpu==1.13.1) (0.16.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow-gpu==1.13.1) (3.1.1)\n",
            "Requirement already satisfied: mock>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow-gpu==1.13.1) (3.0.5)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu==1.13.1) (2.8.0)\n",
            "Requirement already satisfied: numpy==1.15.4 in /usr/local/lib/python3.6/dist-packages (1.15.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpM_A-PW3VMn",
        "colab_type": "code",
        "outputId": "3b79f84f-58b1-4fe5-f912-4f6b526c586d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "! cd dialog_intent_dl && git pull\n",
        "# import pandas as pd\n",
        "# df1 = pd.read_hdf(\"/content/feature_and_vector_seq/8-comm_rosbalt_39_79701_output_10.h5\", engine=\"python\", encoding='cp1251', mode='a')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Already up to date.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bA6kuBnc_4Ij",
        "colab_type": "code",
        "outputId": "d545b479-ac99-4c45-c866-484fa78f044e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import os, random, json, re\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "import numpy as np\n",
        "def load_Doc2Vec_value(str1):\n",
        "  pattern1 = re.compile('\\n')\n",
        "  pattern2 = re.compile(' ')\n",
        "  str2 = pattern1.sub('', str1)\n",
        "  str3 = pattern2.sub(', ', str2)\n",
        "  return json.loads(str3)\n",
        "\n",
        "class SequenceGenerator():\n",
        "    def __init__(self, data_path, intent_index, max_sequence_length, validation_split, only_last=False,\n",
        "                 random_state=None):\n",
        "        self.data_path = data_path\n",
        "        self.max_sequence_length = max_sequence_length\n",
        "        self.intent_index = intent_index\n",
        "        self.num_intents = max(intent_index.values()) + 1\n",
        "        self.only_last = only_last\n",
        "        self.file_list_train, self.file_list_test = self._split(validation_split, random_state=random_state)\n",
        "\n",
        "    def _file2sequence(self, file_and_path):\n",
        "        intent_list = []\n",
        "        embed_list = []\n",
        "        df1 = pd.read_hdf(file_and_path, engine=\"python\", encoding='cp1251')\n",
        "\n",
        "        for key, row in df1.iterrows():\n",
        "            intent = row['Intent analysis']\n",
        "            doc2vec = load_Doc2Vec_value(row[\"Doc2Vec value\"])\n",
        "            if intent:\n",
        "                intent_char = intent[0].lower()\n",
        "            else:\n",
        "                intent_char = \"\"\n",
        "            intent_list.append(self.intent_index[intent_char])\n",
        "            embed_list.append(doc2vec)\n",
        "        return intent_list, embed_list\n",
        "\n",
        "    def _split(self, validation_split, random_state=None):\n",
        "        file_list = os.listdir(self.data_path)\n",
        "        file_list_train, file_list_test = train_test_split(file_list, test_size=validation_split,\n",
        "                                                           random_state=random_state)\n",
        "        return file_list_train, file_list_test\n",
        "\n",
        "    def __build_intent_sequence(self, dialogs_list):\n",
        "        sequence_list = []\n",
        "        for dialog in dialogs_list:\n",
        "            sequence = []\n",
        "            for intent in dialog['Intent analysis'].values:\n",
        "                sequence.append(self.intent_index[intent])\n",
        "            sequence_list.append(sequence)\n",
        "        paded_sequences = pad_sequences(sequence_list, maxlen=self.max_sequence_length)\n",
        "        return paded_sequences\n",
        "\n",
        "    def generate_batch(self, batch_size, subset='training'):\n",
        "        if subset == 'training':\n",
        "            file_list = self.file_list_train\n",
        "        elif subset == 'validation':\n",
        "            file_list = self.file_list_test\n",
        "        f_i = 0\n",
        "\n",
        "        while True:\n",
        "            i = 0\n",
        "            sequence_batch, embed_batch = [], []\n",
        "            while i < batch_size:\n",
        "                if f_i == len(file_list):\n",
        "                    f_i = 0\n",
        "                    random.shuffle(file_list)\n",
        "                file_and_path = os.path.join(self.data_path, file_list[f_i])\n",
        "                intent_sequence, embed_list = self._file2sequence(file_and_path)\n",
        "                if len(intent_sequence) > self.max_sequence_length:\n",
        "                    for ii in range(len(intent_sequence) - self.max_sequence_length + 1):\n",
        "                        sequence_i = intent_sequence[ii:self.max_sequence_length + ii]\n",
        "                        sequence_batch.append(sequence_i)\n",
        "                        embed_list_i = embed_list[ii:self.max_sequence_length + ii]\n",
        "                        embed_batch.append(embed_list_i)\n",
        "                        i += 1\n",
        "                else:\n",
        "                    sequence_batch.append(intent_sequence)\n",
        "                    embed_batch.append(embed_list)\n",
        "                    # print(len(embed_list),len(embed_list[0]))\n",
        "                    i += 1\n",
        "                f_i += 1\n",
        "                # print(\"embed_list\",len(embed_list))\n",
        "                \n",
        "            paded_sequences = pad_sequences(sequence_batch, maxlen=self.max_sequence_length, dtype='float')\n",
        "            paded_embeds = pad_sequences(embed_batch, maxlen=self.max_sequence_length, dtype='float')\n",
        "            # print(paded_embeds)\n",
        "            inputs = paded_sequences[:, :-1]\n",
        "            if self.only_last:\n",
        "                labels = to_categorical(paded_sequences[:, -1:],\n",
        "                                        num_classes=self.num_intents)  # for build_CNN_model build_MLP_model\n",
        "            else:\n",
        "                labels = to_categorical(paded_sequences[:, 1:],\n",
        "                                        num_classes=self.num_intents)  # for build_BiRNN_model build_RNN_model\n",
        "            concat_inputs = np.concatenate((np.expand_dims(inputs,-1), np.array(paded_embeds[:, :-1])), 2)\n",
        "            yield concat_inputs, labels\n",
        "\n",
        "\n",
        "\n",
        "intents = {\"\": 0, \" \": 0, \"а\": 1, \"a\": 1, \"б\": 2, \"в\": 3, \"г\": 4, \"д\": 5,\n",
        "               \"е\": 6, \"e\": 6, \"ж\": 7, \"з\": 8, \"3\": 8, \"и\": 9, \"к\": 10,\n",
        "               \"л\": 11, \"м\": 12, \"н\": 13, \"о\": 14, \"п\": 15,\n",
        "               \"р\": 16, \"с\": 17, \"т\": 18, \"у\": 19, \"ф\": 20,\n",
        "               \"х\": 21, \"ц\": 22, \"ч\": 23, \"ш\": 24, \"щ\": 25}\n",
        "batch_size = 7\n",
        "max_sequence_length = 5\n",
        "validation_split = 0.2\n",
        "random_state = 42\n",
        "sg = SequenceGenerator(\"/content/feature_and_vector_seq\", intents, max_sequence_length, validation_split,\n",
        "                                    only_last=False,\n",
        "                                    random_state=random_state)\n",
        "inputs, labels = next(sg.generate_batch(batch_size, subset='training'))\n",
        "inputs.shape, labels.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((7, 4, 301), (7, 4, 26))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3uevvu23qmS",
        "colab_type": "code",
        "outputId": "36a4f958-06b7-40ee-8eb0-75c3ae768757",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras.layers import Dense, Input, GlobalMaxPooling1D, Flatten, TimeDistributed, Bidirectional\n",
        "from keras.layers import Conv1D, MaxPooling1D, Embedding, LSTM, Dropout, Activation, Permute, Lambda\n",
        "from keras import regularizers\n",
        "from keras.models import Model, Sequential\n",
        "from keras.callbacks import TensorBoard\n",
        "from keras.backend import permute_dimensions, squeeze, expand_dims\n",
        "# from dialog_intent_dl.ipynb.keras_handler.sequence_generator import SequenceGenerator\n",
        "# from dialog_intent_dl.ipynb.keras_handler.sequence_generator import PredictIntent\n",
        "print(tf.__version__)\n",
        "\n",
        "def freeze_session(session, keep_var_names=None, output_names=None, clear_devices=True):\n",
        "    \"\"\"\n",
        "    Freezes the state of a session into a pruned computation graph.\n",
        "    Creates a new computation graph where variable nodes are replaced by\n",
        "    constants taking their current value in the session. The new graph will be\n",
        "    pruned so subgraphs that are not necessary to compute the requested\n",
        "    outputs are removed.\n",
        "    @param session The TensorFlow session to be frozen.\n",
        "    @param keep_var_names A list of variable names that should not be frozen,\n",
        "                          or None to freeze all the variables in the graph.\n",
        "    @param output_names Names of the relevant graph outputs.\n",
        "    @param clear_devices Remove the device directives from the graph for better portability.\n",
        "    @return The frozen graph definition.\n",
        "    \"\"\"\n",
        "    graph = session.graph\n",
        "    with graph.as_default():\n",
        "        freeze_var_names = list(set(v.op.name for v in tf.global_variables()).difference(keep_var_names or []))\n",
        "        output_names = output_names or []\n",
        "        output_names += [v.op.name for v in tf.global_variables()]\n",
        "        input_graph_def = graph.as_graph_def()\n",
        "        if clear_devices:\n",
        "            for node in input_graph_def.node:\n",
        "                node.device = \"\"\n",
        "        frozen_graph = tf.graph_util.convert_variables_to_constants(\n",
        "            session, input_graph_def, output_names, freeze_var_names)\n",
        "        return frozen_graph\n",
        "    \n",
        "def save_model(session, model, name):\n",
        "    frozen_graph = freeze_session(session, output_names=[out.op.name for out in model.outputs])\n",
        "    tf.train.write_graph(frozen_graph, \"../models\", \"{}\".format(name), as_text=False)\n",
        "    \n",
        "class PredictIntent():\n",
        "    intents = {\"\": 0, \" \": 0, \"а\": 1, \"a\": 1, \"б\": 2, \"в\": 3, \"г\": 4, \"д\": 5,\n",
        "               \"е\": 6, \"e\": 6, \"ж\": 7, \"з\": 8, \"3\": 8, \"и\": 9, \"к\": 10,\n",
        "               \"л\": 11, \"м\": 12, \"н\": 13, \"о\": 14, \"п\": 15,\n",
        "               \"р\": 16, \"с\": 17, \"т\": 18, \"у\": 19, \"ф\": 20,\n",
        "               \"х\": 21, \"ц\": 22, \"ч\": 23, \"ш\": 24, \"щ\": 25}\n",
        "    general_intents = {\"\": 0, \" \": 0, \"а\": 1, \"a\": 1, \"б\": 1, \"в\": 1, \"г\": 1, \"д\": 1,  # Информативно-воспроизводящий\n",
        "                       \"е\": 2, \"e\": 2, \"ж\": 2, \"з\": 2, \"3\": 2, \"и\": 2, \"к\": 2,  # Эмотивно-консолидирующий\n",
        "                       \"л\": 3, \"м\": 3, \"н\": 3, \"о\": 3, \"п\": 3,  # Манипулятивный тип, доминирование\n",
        "                       \"р\": 4, \"с\": 4, \"т\": 4, \"у\": 4, \"ф\": 4,  # Волюнтивно-директивный\n",
        "                       \"х\": 5, \"ц\": 5, \"ч\": 5, \"ш\": 5, \"щ\": 5}  # Контрольно-реактивный\n",
        "    batch_size = 13\n",
        "    max_sequence_length = 5\n",
        "    intent_embedding_dim = 10\n",
        "    input_embedding_dim =300\n",
        "    num_units = 30\n",
        "    validation_split = 0.2\n",
        "    random_state = 42\n",
        "    tb = TensorBoard(log_dir='./logs', histogram_freq=0, batch_size=32, write_graph=True, write_grads=False,\n",
        "                     write_images=False, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None)\n",
        "\n",
        "    def __init__(self, is_general=False, data_path = \"../feature_and_vector_seq\"):\n",
        "        self.data_path = data_path\n",
        "        if is_general:\n",
        "            self.intent_index = self.general_intents\n",
        "        else:\n",
        "            self.intent_index = self.intents\n",
        "        self.num_intents = max(self.intent_index.values()) + 1\n",
        "        self.sg = SequenceGenerator(self.data_path, self.intent_index, self.max_sequence_length, self.validation_split,\n",
        "                                    only_last=False,\n",
        "                                    random_state=self.random_state)\n",
        "        print('num_intents', self.num_intents)\n",
        "\n",
        "    def build_CNN_model(self):\n",
        "        embedding_layer = Embedding(self.num_intents,\n",
        "                                    self.intent_embedding_dim,\n",
        "                                    input_length=self.max_sequence_length - 1,\n",
        "                                    trainable=True)\n",
        "\n",
        "        # sequence_input = Input(shape=(self.max_sequence_length - 1,), dtype='int32')\n",
        "        concat_input = Input(shape=(self.max_sequence_length - 1,self.input_embedding_dim+1), dtype='float')\n",
        "        def lambda_fun(x) : \n",
        "          split1, split2 = tf.split(x, [1, 300], 2)\n",
        "          return split1, split2\n",
        "        sequence_input, embedded_input = Lambda(lambda_fun)(concat_input)\n",
        "        # sequence_input = concat_input[:, :, 0]\n",
        "        # embedded_input = concat_input[:, :, 1:]\n",
        "        embedded_sequences = embedding_layer(sequence_input)\n",
        "        #         x_p = permute_dimensions(embedded_sequences, pattern=[0, 2, 1])\n",
        "\n",
        "        x = Conv1D(128, 2, activation='relu', kernel_regularizer=regularizers.l2(0.0001))(embedded_sequences)\n",
        "        x = MaxPooling1D(1)(x)\n",
        "        x = Dropout(0.2)(x)\n",
        "        x = Conv1D(128, 3, activation='relu', kernel_regularizer=regularizers.l2(0.0001))(x)\n",
        "        x = GlobalMaxPooling1D()(x)\n",
        "        x = Dropout(0.2)(x)\n",
        "        x = Dense(128, activation='relu')(x)\n",
        "        preds = Dense(self.num_intents, activation='softmax')(x)\n",
        "\n",
        "        model = Model(concat_input, preds)\n",
        "        model.compile(loss='categorical_crossentropy',\n",
        "                      optimizer='rmsprop',\n",
        "                      metrics=['acc'])\n",
        "        self.model = model\n",
        "        self.sg.only_last = True\n",
        "        return self.model\n",
        "\n",
        "    def build_MLP_model(self):\n",
        "        embedding_layer = Embedding(self.num_intents,\n",
        "                                    self.intent_embedding_dim,\n",
        "                                    input_length=self.max_sequence_length - 1,\n",
        "                                    trainable=True)\n",
        "        embedded_input = Input(shape=(self.max_sequence_length - 1, self.input_embedding_dim), dtype='float')\n",
        "        model = Sequential()\n",
        "        model.add(embedding_layer)\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(128, activation='relu', name=\"Dense1\"\n",
        "                        #                         , activity_regularizer=regularizers.l1(0.009)\n",
        "                        , kernel_regularizer=regularizers.l2(0.0001)\n",
        "                        #                         , bias_regularizer = regularizers.l2(0.009)\n",
        "                        ))  #\n",
        "        #         model.add(Dropout(0.2))\n",
        "        model.add(Dense(self.num_intents, activation='softmax', name=\"Dense2\"\n",
        "                        #                         , activity_regularizer=regularizers.l1(0.009)\n",
        "                        , kernel_regularizer=regularizers.l2(0.0001)\n",
        "                        #                         , bias_regularizer = regularizers.l2(0.009)\n",
        "                        ))\n",
        "        #         model.add(Dropout(0.2))\n",
        "        model.compile(loss='categorical_crossentropy',\n",
        "                      optimizer='rmsprop',\n",
        "                      metrics=['acc'])\n",
        "        self.model = model\n",
        "        self.sg.only_last = True\n",
        "        return self.model\n",
        "\n",
        "    def build_RNN_model(self):\n",
        "        embedding_layer = Embedding(self.num_intents,\n",
        "                                    self.intent_embedding_dim,\n",
        "                                    input_length=self.max_sequence_length - 1,\n",
        "                                    trainable=True)\n",
        "\n",
        "        sequence_input = Input(shape=(self.max_sequence_length - 1,), dtype='int32')\n",
        "        embedded_input = Input(shape=(self.max_sequence_length - 1, self.input_embedding_dim), dtype='float')\n",
        "        embedded_sequences = embedding_layer(sequence_input)\n",
        "\n",
        "        #         x_t = permute_dimensions(embedded_sequences, pattern=[0, 2, 1])\n",
        "        #         print('embedded_sequences',embedded_sequences.shape)\n",
        "        #         print('x_t',x_t.shape)\n",
        "\n",
        "        x = LSTM(self.num_units, dropout=0.2, recurrent_dropout=0.2, return_sequences=True)(embedded_sequences)\n",
        "        x = LSTM(self.num_units, dropout=0.2, recurrent_dropout=0.2, return_sequences=True)(x)\n",
        "        preds = TimeDistributed(Dense(self.num_intents, activation='softmax'))(x)\n",
        "        #         preds = Dense(self.num_intents, activation='softmax')(x)\n",
        "        #         print('preds.shape',preds.shape)\n",
        "\n",
        "        model = Model(sequence_input, embedded_input, preds)\n",
        "        model.compile(loss='categorical_crossentropy',\n",
        "                      optimizer='rmsprop',\n",
        "                      metrics=['acc'])\n",
        "        self.model = model\n",
        "        self.sg.only_last = False\n",
        "        return self.model\n",
        "\n",
        "    def build_BiRNN_model(self):\n",
        "        embedding_layer = Embedding(self.num_intents,\n",
        "                                    self.intent_embedding_dim,\n",
        "                                    input_length=self.max_sequence_length - 1,\n",
        "                                    trainable=True)\n",
        "        embedded_input = Input(shape=(self.max_sequence_length - 1, self.input_embedding_dim), dtype='float')\n",
        "        model = Sequential()\n",
        "        model.add(embedding_layer)\n",
        "        #         model.add(Permute([1, 2]))\n",
        "        model.add(Bidirectional(LSTM(self.num_units, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)))\n",
        "        model.add(Bidirectional(LSTM(self.num_units, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)))\n",
        "        #         model.add(Bidirectional(LSTM(self.num_units, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)))\n",
        "        model.add(\n",
        "            TimeDistributed(Dense(self.num_intents, activation='softmax', name=\"Dense2\"), name=\"TimeDistributed2\"))\n",
        "        model.compile(loss='categorical_crossentropy',\n",
        "                      optimizer='rmsprop',\n",
        "                      metrics=['acc'])\n",
        "        self.model = model\n",
        "        return self.model\n",
        "\n",
        "    def RNN_attention(self):\n",
        "        embedding_layer = Embedding(self.num_intents,\n",
        "                                    self.intent_embedding_dim,\n",
        "                                    input_length=self.max_sequence_length - 1,\n",
        "                                    trainable=True)\n",
        "        embedded_input = Input(shape=(self.max_sequence_length - 1, self.input_embedding_dim), dtype='float')\n",
        "        model1 = Sequential()\n",
        "        model1.add(embedding_layer)\n",
        "        model1.add(LSTM(self.num_units, return_sequences=True))\n",
        "\n",
        "        model2 = Sequential()\n",
        "        model2.add(Dense(input_dim=input_dim, output_dim=step))\n",
        "        model2.add(Activation('softmax'))  # Learn a probability distribution over each  step.\n",
        "        # Reshape to match LSTM's output shape, so that we can do element-wise multiplication.\n",
        "        model2.add(RepeatVector(self.num_units))\n",
        "        model2.add(Permute(2, 1))\n",
        "\n",
        "        model = Sequential()\n",
        "        model.add(\n",
        "            Merge([model1, model2], 'mul'))  # Multiply each element with corresponding weight a[i][j][k] * b[i][j]\n",
        "        model.add(TimeDistributedMergeinput_dim('sum'))  # Sum the weighted elements.\n",
        "\n",
        "        attention = Dense(1, activation='tanh')(activations)\n",
        "        attention = Flatten()(attention)\n",
        "        attention = Activation('softmax')(attention)\n",
        "        attention = RepeatVector(units)(attention)\n",
        "        attention = Permute([2, 1])(attention)\n",
        "\n",
        "        sent_representation = merge([activations, attention], mode='mul')\n",
        "        model.compile(loss='categorical_crossentropy',\n",
        "                      optimizer='rmsprop',\n",
        "                      metrics=['acc'])\n",
        "        self.model = model\n",
        "        self.sg = SequenceGenerator(self.data_path, self.intent_index, self.max_sequence_length, self.validation_split,\n",
        "                                    only_last=False)\n",
        "        return self.model\n",
        "\n",
        "    def fit_generator(self, epochs):\n",
        "        history_nn = self.model.fit_generator(\n",
        "            generator=self.sg.generate_batch(self.batch_size, subset='training'),\n",
        "            steps_per_epoch=len(os.listdir(self.data_path)) * (1 - self.validation_split) // self.batch_size,\n",
        "            epochs=epochs,\n",
        "            validation_data=self.sg.generate_batch(self.batch_size, subset='validation'),\n",
        "            validation_steps=len(os.listdir(self.data_path)) * self.validation_split // self.batch_size + 1,\n",
        "            callbacks=[self.tb]\n",
        "        )\n",
        "        return history_nn"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.13.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ErhQ1GlJcBZp",
        "colab_type": "code",
        "outputId": "a19fd509-708e-4fc2-fa5f-4b825ee2b62d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "PATH_TO_DATASET = \"/content/feature_and_vector_seq\"\n",
        "pi = PredictIntent(data_path = PATH_TO_DATASET, is_general = False)\n",
        "pi.batch_size = 7\n",
        "pi.max_sequence_length = 5\n",
        "pi.intent_embedding_dim = 10\n",
        "pi.num_units = 30\n",
        "pi.validation_split = 0.2\n",
        "pi.random_state = 42"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "num_intents 26\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfBfvvwaccMb",
        "colab_type": "code",
        "outputId": "f9a4e8e8-9c0d-4bc9-de78-0c854834f0fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        }
      },
      "source": [
        "pi.build_CNN_model()\n",
        "history_CNN = pi.fit_generator(epochs = 1)\n",
        "from keras import backend as K\n",
        "save_model(K.get_session(), pi.model, 'history_CNN.pb')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-94-262914b709c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_CNN_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mhistory_CNN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'history_CNN.pb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-92-079a93d46682>\u001b[0m in \u001b[0;36mbuild_CNN_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_intents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'softmax'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconcat_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         model.compile(loss='categorical_crossentropy',\n\u001b[1;32m    102\u001b[0m                       \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rmsprop'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/network.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m                 'inputs' in kwargs and 'outputs' in kwargs):\n\u001b[1;32m     93\u001b[0m             \u001b[0;31m# Graph network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_graph_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0;31m# Subclassed network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/network.py\u001b[0m in \u001b[0;36m_init_graph_network\u001b[0;34m(self, inputs, outputs, name, **kwargs)\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0;31m# Keep track of the network's nodes and layers.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         nodes, nodes_by_depth, layers, layers_by_depth = _map_graph_network(\n\u001b[0;32m--> 237\u001b[0;31m             self.inputs, self.outputs)\n\u001b[0m\u001b[1;32m    238\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_network_nodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nodes_by_depth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnodes_by_depth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/network.py\u001b[0m in \u001b[0;36m_map_graph_network\u001b[0;34m(inputs, outputs)\u001b[0m\n\u001b[1;32m   1419\u001b[0m                   \u001b[0mlayer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m                   \u001b[0mnode_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1421\u001b[0;31m                   tensor_index=tensor_index)\n\u001b[0m\u001b[1;32m   1422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1423\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnodes_in_decreasing_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/network.py\u001b[0m in \u001b[0;36mbuild_map\u001b[0;34m(tensor, finished_nodes, nodes_in_progress, layer, node_index, tensor_index)\u001b[0m\n\u001b[1;32m   1406\u001b[0m             \u001b[0mtensor_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1407\u001b[0m             build_map(x, finished_nodes, nodes_in_progress, layer,\n\u001b[0;32m-> 1408\u001b[0;31m                       node_index, tensor_index)\n\u001b[0m\u001b[1;32m   1409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1410\u001b[0m         \u001b[0mfinished_nodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/network.py\u001b[0m in \u001b[0;36mbuild_map\u001b[0;34m(tensor, finished_nodes, nodes_in_progress, layer, node_index, tensor_index)\u001b[0m\n\u001b[1;32m   1406\u001b[0m             \u001b[0mtensor_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1407\u001b[0m             build_map(x, finished_nodes, nodes_in_progress, layer,\n\u001b[0;32m-> 1408\u001b[0;31m                       node_index, tensor_index)\n\u001b[0m\u001b[1;32m   1409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1410\u001b[0m         \u001b[0mfinished_nodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/network.py\u001b[0m in \u001b[0;36mbuild_map\u001b[0;34m(tensor, finished_nodes, nodes_in_progress, layer, node_index, tensor_index)\u001b[0m\n\u001b[1;32m   1406\u001b[0m             \u001b[0mtensor_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1407\u001b[0m             build_map(x, finished_nodes, nodes_in_progress, layer,\n\u001b[0;32m-> 1408\u001b[0;31m                       node_index, tensor_index)\n\u001b[0m\u001b[1;32m   1409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1410\u001b[0m         \u001b[0mfinished_nodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/network.py\u001b[0m in \u001b[0;36mbuild_map\u001b[0;34m(tensor, finished_nodes, nodes_in_progress, layer, node_index, tensor_index)\u001b[0m\n\u001b[1;32m   1406\u001b[0m             \u001b[0mtensor_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1407\u001b[0m             build_map(x, finished_nodes, nodes_in_progress, layer,\n\u001b[0;32m-> 1408\u001b[0;31m                       node_index, tensor_index)\n\u001b[0m\u001b[1;32m   1409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1410\u001b[0m         \u001b[0mfinished_nodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/network.py\u001b[0m in \u001b[0;36mbuild_map\u001b[0;34m(tensor, finished_nodes, nodes_in_progress, layer, node_index, tensor_index)\u001b[0m\n\u001b[1;32m   1406\u001b[0m             \u001b[0mtensor_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1407\u001b[0m             build_map(x, finished_nodes, nodes_in_progress, layer,\n\u001b[0;32m-> 1408\u001b[0;31m                       node_index, tensor_index)\n\u001b[0m\u001b[1;32m   1409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1410\u001b[0m         \u001b[0mfinished_nodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/network.py\u001b[0m in \u001b[0;36mbuild_map\u001b[0;34m(tensor, finished_nodes, nodes_in_progress, layer, node_index, tensor_index)\u001b[0m\n\u001b[1;32m   1406\u001b[0m             \u001b[0mtensor_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1407\u001b[0m             build_map(x, finished_nodes, nodes_in_progress, layer,\n\u001b[0;32m-> 1408\u001b[0;31m                       node_index, tensor_index)\n\u001b[0m\u001b[1;32m   1409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1410\u001b[0m         \u001b[0mfinished_nodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/network.py\u001b[0m in \u001b[0;36mbuild_map\u001b[0;34m(tensor, finished_nodes, nodes_in_progress, layer, node_index, tensor_index)\u001b[0m\n\u001b[1;32m   1406\u001b[0m             \u001b[0mtensor_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1407\u001b[0m             build_map(x, finished_nodes, nodes_in_progress, layer,\n\u001b[0;32m-> 1408\u001b[0;31m                       node_index, tensor_index)\n\u001b[0m\u001b[1;32m   1409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1410\u001b[0m         \u001b[0mfinished_nodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/network.py\u001b[0m in \u001b[0;36mbuild_map\u001b[0;34m(tensor, finished_nodes, nodes_in_progress, layer, node_index, tensor_index)\u001b[0m\n\u001b[1;32m   1406\u001b[0m             \u001b[0mtensor_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1407\u001b[0m             build_map(x, finished_nodes, nodes_in_progress, layer,\n\u001b[0;32m-> 1408\u001b[0;31m                       node_index, tensor_index)\n\u001b[0m\u001b[1;32m   1409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1410\u001b[0m         \u001b[0mfinished_nodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/network.py\u001b[0m in \u001b[0;36mbuild_map\u001b[0;34m(tensor, finished_nodes, nodes_in_progress, layer, node_index, tensor_index)\u001b[0m\n\u001b[1;32m   1406\u001b[0m             \u001b[0mtensor_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1407\u001b[0m             build_map(x, finished_nodes, nodes_in_progress, layer,\n\u001b[0;32m-> 1408\u001b[0;31m                       node_index, tensor_index)\n\u001b[0m\u001b[1;32m   1409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1410\u001b[0m         \u001b[0mfinished_nodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/network.py\u001b[0m in \u001b[0;36mbuild_map\u001b[0;34m(tensor, finished_nodes, nodes_in_progress, layer, node_index, tensor_index)\u001b[0m\n\u001b[1;32m   1378\u001b[0m             \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0ma\u001b[0m \u001b[0mcycle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mdetected\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1379\u001b[0m         \"\"\"\n\u001b[0;32m-> 1380\u001b[0;31m         \u001b[0mnode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inbound_nodes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnode_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1382\u001b[0m         \u001b[0;31m# Prevent cycles.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute '_inbound_nodes'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEx-F_7NATcd",
        "colab_type": "code",
        "outputId": "7a512f64-51dc-4ebc-cd3c-ff6cf95b8be5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        }
      },
      "source": [
        "pi.RNN_attention()\n",
        "history_BiRNN = pi.fit_generator(epochs = 1)\n",
        "from keras import backend as K\n",
        "save_model(K.get_session(), pi.model, 'RNN_attention.pb')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-b18b07f2f5da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRNN_attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mhistory_BiRNN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'RNN_attention.pb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-c9215bde4779>\u001b[0m in \u001b[0;36mRNN_attention\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0mmodel2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m         \u001b[0mmodel2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m         \u001b[0mmodel2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mActivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'softmax'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Learn a probability distribution over each  step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0;31m# Reshape to match LSTM's output shape, so that we can do element-wise multiplication.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'input_dim' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUqWDZ4LwHAX",
        "colab_type": "code",
        "outputId": "964703ef-b7a2-45f1-c95c-eabd8262699c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 498
        }
      },
      "source": [
        "§import pandas as pd\n",
        "import re, json\n",
        "\n",
        "def load_Doc2Vec_value(str1):\n",
        "  pattern1 = re.compile('\\n')\n",
        "  pattern2 = re.compile(' ')\n",
        "  str2 = pattern1.sub('', str1)\n",
        "  str3 = pattern2.sub(', ', str2)\n",
        "  return json.loads(str3)\n",
        "\n",
        "file_and_path = \"/content/feature_and_vector_seq/1-comm_inosmi_111_192344_output_10.h5\"\n",
        "df1 = pd.read_hdf(file_and_path, engine=\"python\", encoding='cp1251')\n",
        "df1[\"Doc2Vec\"] = df1[\"Doc2Vec value\"].apply(load_Doc2Vec_value)\n",
        "df1\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID of comment</th>\n",
              "      <th>ID of post</th>\n",
              "      <th>Likes</th>\n",
              "      <th>Intent analysis</th>\n",
              "      <th>Content analysis</th>\n",
              "      <th>Distance to parent</th>\n",
              "      <th>Distance to post</th>\n",
              "      <th>Doc2Vec value</th>\n",
              "      <th>Doc2Vec</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>192344</td>\n",
              "      <td>192344</td>\n",
              "      <td>0</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "      <td>1.1102230246251565e-16</td>\n",
              "      <td>[0.001903 0.008161 0.003022 0.021471 0.001911 ...</td>\n",
              "      <td>[0.001903, 0.008161, 0.003022, 0.021471, 0.001...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>192366</td>\n",
              "      <td>192344</td>\n",
              "      <td>0</td>\n",
              "      <td>м</td>\n",
              "      <td>1</td>\n",
              "      <td>0.9047975317843413</td>\n",
              "      <td>0.9047975317843413</td>\n",
              "      <td>[0.015871 0.063059 0.027442 0.005918 0.067336 ...</td>\n",
              "      <td>[0.015871, 0.063059, 0.027442, 0.005918, 0.067...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>192369</td>\n",
              "      <td>192344</td>\n",
              "      <td>1</td>\n",
              "      <td>а</td>\n",
              "      <td>3</td>\n",
              "      <td>0.9263160364648992</td>\n",
              "      <td>0.949031225401245</td>\n",
              "      <td>[-0.050479 -0.020054 0.010354 0.009482 -0.0072...</td>\n",
              "      <td>[-0.050479, -0.020054, 0.010354, 0.009482, -0....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>192370</td>\n",
              "      <td>192344</td>\n",
              "      <td>0</td>\n",
              "      <td>ч</td>\n",
              "      <td>3</td>\n",
              "      <td>0.8536771566133271</td>\n",
              "      <td>1.0161022628113872</td>\n",
              "      <td>[-0.003400 0.004530 -0.011841 0.009115 0.01707...</td>\n",
              "      <td>[-0.0034, 0.00453, -0.011841, 0.009115, 0.0170...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>192371</td>\n",
              "      <td>192344</td>\n",
              "      <td>1</td>\n",
              "      <td>н</td>\n",
              "      <td>4</td>\n",
              "      <td>0.4652375928796576</td>\n",
              "      <td>1.1818890054062847</td>\n",
              "      <td>[-0.017641 0.020937 -0.043087 0.017828 0.03717...</td>\n",
              "      <td>[-0.017641, 0.020937, -0.043087, 0.017828, 0.0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>192372</td>\n",
              "      <td>192344</td>\n",
              "      <td>0</td>\n",
              "      <td>р</td>\n",
              "      <td>4</td>\n",
              "      <td>0.5322816918635409</td>\n",
              "      <td>0.918104637002647</td>\n",
              "      <td>[0.014156 0.003831 0.001797 0.009888 0.004583 ...</td>\n",
              "      <td>[0.014156, 0.003831, 0.001797, 0.009888, 0.004...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>192377</td>\n",
              "      <td>192344</td>\n",
              "      <td>0</td>\n",
              "      <td>ф</td>\n",
              "      <td>3</td>\n",
              "      <td>0.9230922955341927</td>\n",
              "      <td>0.5331857866510279</td>\n",
              "      <td>[-0.026679 -0.030724 0.026273 0.014249 0.00759...</td>\n",
              "      <td>[-0.026679, -0.030724, 0.026273, 0.014249, 0.0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>192381</td>\n",
              "      <td>192344</td>\n",
              "      <td>0</td>\n",
              "      <td>ч</td>\n",
              "      <td>4</td>\n",
              "      <td>0.8990398151377282</td>\n",
              "      <td>0.9101620933407938</td>\n",
              "      <td>[0.008495 0.029132 -0.002679 -0.007744 0.01133...</td>\n",
              "      <td>[0.008495, 0.029132, -0.002679, -0.007744, 0.0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>192395</td>\n",
              "      <td>192344</td>\n",
              "      <td>1</td>\n",
              "      <td>ч</td>\n",
              "      <td>3</td>\n",
              "      <td>1.0171192821636792</td>\n",
              "      <td>0.7188240607558654</td>\n",
              "      <td>[-0.009065 -0.010122 0.035569 -0.002556 -0.003...</td>\n",
              "      <td>[-0.009065, -0.010122, 0.035569, -0.002556, -0...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  ...\n",
              "0 ...\n",
              "1 ...\n",
              "2 ...\n",
              "3 ...\n",
              "4 ...\n",
              "5 ...\n",
              "6 ...\n",
              "7 ...\n",
              "8 ...\n",
              "\n",
              "[9 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85_Hp2tYcj9I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_U4opvW6xx4N",
        "colab_type": "code",
        "outputId": "a872bdd2-8603-4af6-f21e-41dd2740abdd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 378
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "data = np.random.rand(200,2)\n",
        "expected = np.random.randint(2, size=200).reshape(-1,1)\n",
        "\n",
        "dataFrame = pd.DataFrame(data, columns = ['a','b'])\n",
        "expectedFrame = pd.DataFrame(expected, columns = ['expected'])\n",
        "\n",
        "dataFrameTrain, dataFrameTest = dataFrame[:100],dataFrame[-100:]\n",
        "expectedFrameTrain, expectedFrameTest = expectedFrame[:100],expectedFrame[-100:]\n",
        "\n",
        "def generator(X_data, y_data, batch_size):\n",
        "\n",
        "  samples_per_epoch = X_data.shape[0]\n",
        "  number_of_batches = samples_per_epoch/batch_size\n",
        "  counter=0\n",
        "  while 1:\n",
        "    X_batch = np.array(X_data[batch_size*counter:batch_size*(counter+1)]).astype('float32')\n",
        "    y_batch = np.array(y_data[batch_size*counter:batch_size*(counter+1)]).astype('float32')\n",
        "    counter += 1\n",
        "    yield (X_batch,X_batch),y_batch\n",
        "\n",
        "    #restart counter to yeild data in the next epoch as well\n",
        "    if counter >= number_of_batches:\n",
        "        counter = 0\n",
        "\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Dropout, Activation, Flatten, Reshape\n",
        "from keras.layers.convolutional import Convolution1D, Convolution2D, MaxPooling2D\n",
        "from keras.utils import np_utils\n",
        "\n",
        "inputs1 = Input(shape=(dataFrame.shape[1],))\n",
        "# inputs2 = Input(shape=(dataFrame.shape[1],))\n",
        "x = Dense(12, activation='relu', input_dim=dataFrame.shape[1])(inputs1)\n",
        "preds = Dense(1, activation='sigmoid')(x)\n",
        "model = Model(inputs1, preds)\n",
        "# model = Sequential()\n",
        "# model.add(Dense(12, activation='relu', input_dim=dataFrame.shape[1]))\n",
        "# model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adadelta', metrics=['accuracy'])\n",
        "\n",
        "#Train the model using generator vs using the full batch\n",
        "batch_size = 8\n",
        "\n",
        "model.fit_generator(generator(dataFrameTrain,expectedFrameTrain,batch_size), epochs=3,steps_per_epoch = dataFrame.shape[0]/batch_size, validation_data=generator(dataFrameTest,expectedFrameTest,batch_size*2),validation_steps=dataFrame.shape[0]/batch_size*2)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-a0ff09a28986>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataFrameTrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mexpectedFrameTrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msteps_per_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataFrameTest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mexpectedFrameTest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1656\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1657\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1658\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1659\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    206\u001b[0m                     \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m                     \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m                 \u001b[0;31m# build batch logs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                 \u001b[0mbatch_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'batch'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'size'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'shape'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZhjTP-mhzkwD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}