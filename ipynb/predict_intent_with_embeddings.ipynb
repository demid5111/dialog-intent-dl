{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PredictOnColab.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/demid5111/dialog-intent-dl/blob/master/ipynb/predict_intent_with_embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tE1UJlzMur_k",
        "colab_type": "code",
        "outputId": "43385b1b-6939-4e2e-838a-14e23bf8cbc7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anjRwJ1o0yqo",
        "colab_type": "code",
        "outputId": "e4645bf2-1db7-4d29-af94-eaf8bb2a9ba7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "! tar -xzf \"/content/drive/My Drive/Projects/RFFI2016/dataset.tar.gz\" -C /content\n",
        "! ls \"/content/feature_and_vector_seq\" | wc -l"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "13156\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWDAX-g82iqa",
        "colab_type": "code",
        "outputId": "48a0424d-5722-4465-be30-943d5db0f546",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "! git clone https://github.com/demid5111/dialog-intent-dl.git\n",
        "! mv dialog-intent-dl dialog_intent_dl\n",
        "! pip install -r dialog_intent_dl/requirements.txt\n",
        "! pip install tensorflow-gpu==1.13.1\n",
        "! pip install numpy==1.15.4\n",
        "# restart runtime\n",
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'dialog-intent-dl'...\n",
            "remote: Enumerating objects: 41, done.\u001b[K\n",
            "remote: Counting objects:   2% (1/41)\u001b[K\rremote: Counting objects:   4% (2/41)\u001b[K\rremote: Counting objects:   7% (3/41)\u001b[K\rremote: Counting objects:   9% (4/41)\u001b[K\rremote: Counting objects:  12% (5/41)\u001b[K\rremote: Counting objects:  14% (6/41)\u001b[K\rremote: Counting objects:  17% (7/41)\u001b[K\rremote: Counting objects:  19% (8/41)\u001b[K\rremote: Counting objects:  21% (9/41)\u001b[K\rremote: Counting objects:  24% (10/41)\u001b[K\rremote: Counting objects:  26% (11/41)\u001b[K\rremote: Counting objects:  29% (12/41)\u001b[K\rremote: Counting objects:  31% (13/41)\u001b[K\rremote: Counting objects:  34% (14/41)\u001b[K\rremote: Counting objects:  36% (15/41)\u001b[K\rremote: Counting objects:  39% (16/41)\u001b[K\rremote: Counting objects:  41% (17/41)\u001b[K\rremote: Counting objects:  43% (18/41)\u001b[K\rremote: Counting objects:  46% (19/41)\u001b[K\rremote: Counting objects:  48% (20/41)\u001b[K\rremote: Counting objects:  51% (21/41)\u001b[K\rremote: Counting objects:  53% (22/41)\u001b[K\rremote: Counting objects:  56% (23/41)\u001b[K\rremote: Counting objects:  58% (24/41)\u001b[K\rremote: Counting objects:  60% (25/41)\u001b[K\rremote: Counting objects:  63% (26/41)\u001b[K\rremote: Counting objects:  65% (27/41)\u001b[K\rremote: Counting objects:  68% (28/41)\u001b[K\rremote: Counting objects:  70% (29/41)\u001b[K\rremote: Counting objects:  73% (30/41)\u001b[K\rremote: Counting objects:  75% (31/41)\u001b[K\rremote: Counting objects:  78% (32/41)\u001b[K\rremote: Counting objects:  80% (33/41)\u001b[K\rremote: Counting objects:  82% (34/41)\u001b[K\rremote: Counting objects:  85% (35/41)\u001b[K\rremote: Counting objects:  87% (36/41)\u001b[K\rremote: Counting objects:  90% (37/41)\u001b[K\rremote: Counting objects:  92% (38/41)\u001b[K\rremote: Counting objects:  95% (39/41)\u001b[K\rremote: Counting objects:  97% (40/41)\u001b[K\rremote: Counting objects: 100% (41/41)\u001b[K\rremote: Counting objects: 100% (41/41), done.\u001b[K\n",
            "remote: Compressing objects: 100% (34/34), done.\u001b[K\n",
            "remote: Total 189 (delta 22), reused 16 (delta 7), pack-reused 148\u001b[K\n",
            "Receiving objects: 100% (189/189), 186.94 KiB | 506.00 KiB/s, done.\n",
            "Resolving deltas: 100% (94/94), done.\n",
            "Collecting nltk==3.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/09/3b1755d528ad9156ee7243d52aa5cd2b809ef053a0f31b53d92853dd653a/nltk-3.3.0.zip (1.4MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4MB 2.5MB/s \n",
            "\u001b[?25hCollecting gevent==1.3.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ef/71/48aa308c1a92475e89afbd1a45d6978ac69b007169c761f048a5c9336f2d/gevent-1.3.4-cp36-cp36m-manylinux1_x86_64.whl (4.5MB)\n",
            "\u001b[K     |████████████████████████████████| 4.5MB 35.3MB/s \n",
            "\u001b[?25hCollecting gensim==3.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/33/33/df6cb7acdcec5677ed130f4800f67509d24dbec74a03c329fcbf6b0864f0/gensim-3.4.0-cp36-cp36m-manylinux1_x86_64.whl (22.6MB)\n",
            "\u001b[K     |████████████████████████████████| 22.6MB 398kB/s \n",
            "\u001b[?25hCollecting networkx==2.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/11/42/f951cc6838a4dff6ce57211c4d7f8444809ccbe2134179950301e5c4c83c/networkx-2.1.zip (1.6MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6MB 18.6MB/s \n",
            "\u001b[?25hCollecting numpy==1.15.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ff/7f/9d804d2348471c67a7d8b5f84f9bc59fd1cefa148986f2b74552f8573555/numpy-1.15.4-cp36-cp36m-manylinux1_x86_64.whl (13.9MB)\n",
            "\u001b[K     |████████████████████████████████| 13.9MB 22.7MB/s \n",
            "\u001b[?25hCollecting pandas==0.23.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/57/eb/6ab533ea8e35e7dd159af6922ac1123d4565d89f3926ad9a6aa46530978f/pandas-0.23.1-cp36-cp36m-manylinux1_x86_64.whl (11.8MB)\n",
            "\u001b[K     |████████████████████████████████| 11.8MB 22.3MB/s \n",
            "\u001b[?25hCollecting scipy==1.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/0b/f163da98d3a01b3e0ef1cab8dd2123c34aee2bafbb1c5bffa354cc8a1730/scipy-1.1.0-cp36-cp36m-manylinux1_x86_64.whl (31.2MB)\n",
            "\u001b[K     |████████████████████████████████| 31.2MB 1.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: xlrd==1.1.0 in /usr/local/lib/python3.6/dist-packages (from -r dialog_intent_dl/requirements.txt (line 8)) (1.1.0)\n",
            "Collecting xlsxwriter==1.0.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/33/50/136b801d106fcebb2428a764e5c599e020d8227a3623db078e05eb4793a5/XlsxWriter-1.0.5-py2.py3-none-any.whl (142kB)\n",
            "\u001b[K     |████████████████████████████████| 143kB 49.8MB/s \n",
            "\u001b[?25hCollecting tensorflow-gpu==1.13.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7b/b1/0ad4ae02e17ddd62109cd54c291e311c4b5fd09b4d0678d3d6ce4159b0f0/tensorflow_gpu-1.13.1-cp36-cp36m-manylinux1_x86_64.whl (345.2MB)\n",
            "\u001b[K     |████████████████████████████████| 345.2MB 50kB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk==3.3.0->-r dialog_intent_dl/requirements.txt (line 1)) (1.12.0)\n",
            "Requirement already satisfied: greenlet>=0.4.13; platform_python_implementation == \"CPython\" in /usr/local/lib/python3.6/dist-packages (from gevent==1.3.4->-r dialog_intent_dl/requirements.txt (line 2)) (0.4.15)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim==3.4.0->-r dialog_intent_dl/requirements.txt (line 3)) (1.8.4)\n",
            "Requirement already satisfied: decorator>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from networkx==2.1.0->-r dialog_intent_dl/requirements.txt (line 4)) (4.4.1)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas==0.23.1->-r dialog_intent_dl/requirements.txt (line 6)) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python3.6/dist-packages (from pandas==0.23.1->-r dialog_intent_dl/requirements.txt (line 6)) (2.6.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1->-r dialog_intent_dl/requirements.txt (line 10)) (1.1.0)\n",
            "Collecting tensorflow-estimator<1.14.0rc0,>=1.13.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/48/13f49fc3fa0fdf916aa1419013bb8f2ad09674c275b4046d5ee669a46873/tensorflow_estimator-1.13.0-py2.py3-none-any.whl (367kB)\n",
            "\u001b[K     |████████████████████████████████| 368kB 40.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1->-r dialog_intent_dl/requirements.txt (line 10)) (1.15.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1->-r dialog_intent_dl/requirements.txt (line 10)) (0.8.0)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1->-r dialog_intent_dl/requirements.txt (line 10)) (0.8.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1->-r dialog_intent_dl/requirements.txt (line 10)) (1.1.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1->-r dialog_intent_dl/requirements.txt (line 10)) (3.10.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1->-r dialog_intent_dl/requirements.txt (line 10)) (0.2.2)\n",
            "Collecting tensorboard<1.14.0,>=1.13.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/39/bdd75b08a6fba41f098b6cb091b9e8c7a80e1b4d679a581a0ccd17b10373/tensorboard-1.13.1-py3-none-any.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 23.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1->-r dialog_intent_dl/requirements.txt (line 10)) (0.33.6)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1->-r dialog_intent_dl/requirements.txt (line 10)) (1.0.8)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim==3.4.0->-r dialog_intent_dl/requirements.txt (line 3)) (1.10.7)\n",
            "Requirement already satisfied: boto>=2.32 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim==3.4.0->-r dialog_intent_dl/requirements.txt (line 3)) (2.49.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim==3.4.0->-r dialog_intent_dl/requirements.txt (line 3)) (2.21.0)\n",
            "Collecting mock>=2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/05/d2/f94e68be6b17f46d2c353564da56e6fb89ef09faeeff3313a046cb810ca9/mock-3.0.5-py2.py3-none-any.whl\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu==1.13.1->-r dialog_intent_dl/requirements.txt (line 10)) (41.4.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow-gpu==1.13.1->-r dialog_intent_dl/requirements.txt (line 10)) (0.16.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow-gpu==1.13.1->-r dialog_intent_dl/requirements.txt (line 10)) (3.1.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu==1.13.1->-r dialog_intent_dl/requirements.txt (line 10)) (2.8.0)\n",
            "Requirement already satisfied: botocore<1.14.0,>=1.13.7 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim==3.4.0->-r dialog_intent_dl/requirements.txt (line 3)) (1.13.7)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim==3.4.0->-r dialog_intent_dl/requirements.txt (line 3)) (0.9.4)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim==3.4.0->-r dialog_intent_dl/requirements.txt (line 3)) (0.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim==3.4.0->-r dialog_intent_dl/requirements.txt (line 3)) (2019.9.11)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim==3.4.0->-r dialog_intent_dl/requirements.txt (line 3)) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim==3.4.0->-r dialog_intent_dl/requirements.txt (line 3)) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim==3.4.0->-r dialog_intent_dl/requirements.txt (line 3)) (1.24.3)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.7->boto3->smart-open>=1.2.1->gensim==3.4.0->-r dialog_intent_dl/requirements.txt (line 3)) (0.15.2)\n",
            "Building wheels for collected packages: nltk, networkx\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.3-cp36-none-any.whl size=1394469 sha256=9e50e62e19a38e7a81f3c95cd9b76f0752c99bbf1bdae64bdf41f0d95e6ceb44\n",
            "  Stored in directory: /root/.cache/pip/wheels/d1/ab/40/3bceea46922767e42986aef7606a600538ca80de6062dc266c\n",
            "  Building wheel for networkx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for networkx: filename=networkx-2.1-py2.py3-none-any.whl size=1447766 sha256=a5cc91afeded8e565872153975aeec3b3c132e82aec3cd92133fff7c6d508df3\n",
            "  Stored in directory: /root/.cache/pip/wheels/44/c0/34/6f98693a554301bdb405f8d65d95bbcd3e50180cbfdd98a94e\n",
            "Successfully built nltk networkx\n",
            "\u001b[31mERROR: tensorflow 1.15.0 has requirement numpy<2.0,>=1.16.0, but you'll have numpy 1.15.4 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 1.15.0 has requirement tensorboard<1.16.0,>=1.15.0, but you'll have tensorboard 1.13.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 1.15.0 has requirement tensorflow-estimator==1.15.1, but you'll have tensorflow-estimator 1.13.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: plotnine 0.5.1 has requirement pandas>=0.23.4, but you'll have pandas 0.23.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: mizani 0.5.4 has requirement pandas>=0.23.4, but you'll have pandas 0.23.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement pandas~=0.25.0; python_version >= \"3.0\", but you'll have pandas 0.23.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: fbprophet 0.5 has requirement pandas>=0.23.4, but you'll have pandas 0.23.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: nltk, gevent, numpy, scipy, gensim, networkx, pandas, xlsxwriter, mock, tensorflow-estimator, tensorboard, tensorflow-gpu\n",
            "  Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "  Found existing installation: gevent 1.4.0\n",
            "    Uninstalling gevent-1.4.0:\n",
            "      Successfully uninstalled gevent-1.4.0\n",
            "  Found existing installation: numpy 1.17.3\n",
            "    Uninstalling numpy-1.17.3:\n",
            "      Successfully uninstalled numpy-1.17.3\n",
            "  Found existing installation: scipy 1.3.1\n",
            "    Uninstalling scipy-1.3.1:\n",
            "      Successfully uninstalled scipy-1.3.1\n",
            "  Found existing installation: gensim 3.6.0\n",
            "    Uninstalling gensim-3.6.0:\n",
            "      Successfully uninstalled gensim-3.6.0\n",
            "  Found existing installation: networkx 2.4\n",
            "    Uninstalling networkx-2.4:\n",
            "      Successfully uninstalled networkx-2.4\n",
            "  Found existing installation: pandas 0.25.3\n",
            "    Uninstalling pandas-0.25.3:\n",
            "      Successfully uninstalled pandas-0.25.3\n",
            "  Found existing installation: tensorflow-estimator 1.15.1\n",
            "    Uninstalling tensorflow-estimator-1.15.1:\n",
            "      Successfully uninstalled tensorflow-estimator-1.15.1\n",
            "  Found existing installation: tensorboard 1.15.0\n",
            "    Uninstalling tensorboard-1.15.0:\n",
            "      Successfully uninstalled tensorboard-1.15.0\n",
            "Successfully installed gensim-3.4.0 gevent-1.3.4 mock-3.0.5 networkx-2.1 nltk-3.3 numpy-1.15.4 pandas-0.23.1 scipy-1.1.0 tensorboard-1.13.1 tensorflow-estimator-1.13.0 tensorflow-gpu-1.13.1 xlsxwriter-1.0.5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy",
                  "pandas"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow-gpu==1.13.1 in /usr/local/lib/python3.6/dist-packages (1.13.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (0.33.6)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (0.8.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (1.1.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (0.8.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (1.15.4)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (1.1.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (1.0.8)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (0.2.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (1.15.0)\n",
            "Requirement already satisfied: tensorboard<1.14.0,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (1.13.1)\n",
            "Requirement already satisfied: tensorflow-estimator<1.14.0rc0,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (1.13.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (1.12.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu==1.13.1) (41.4.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu==1.13.1) (2.8.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow-gpu==1.13.1) (3.1.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow-gpu==1.13.1) (0.16.0)\n",
            "Requirement already satisfied: mock>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow-gpu==1.13.1) (3.0.5)\n",
            "Requirement already satisfied: numpy==1.15.4 in /usr/local/lib/python3.6/dist-packages (1.15.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpM_A-PW3VMn",
        "colab_type": "code",
        "outputId": "9762bcd5-f686-4dd3-8b31-bd58068e2a80",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "! cd dialog_intent_dl && git pull\n",
        "# import pandas as pd\n",
        "# df1 = pd.read_hdf(\"/content/feature_and_vector_seq/8-comm_rosbalt_39_79701_output_10.h5\", engine=\"python\", encoding='cp1251', mode='a')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Already up to date.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bA6kuBnc_4Ij",
        "colab_type": "code",
        "outputId": "4b360e12-2f3d-4e84-8191-e2f1344ad12c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 97
        }
      },
      "source": [
        "import os, random, json, re\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "import numpy as np\n",
        "def load_Doc2Vec_value(str1):\n",
        "  pattern1 = re.compile('\\n')\n",
        "  pattern2 = re.compile(' ')\n",
        "  str2 = pattern1.sub('', str1)\n",
        "  str3 = pattern2.sub(', ', str2)\n",
        "  return json.loads(str3)\n",
        "\n",
        "class SequenceGenerator():\n",
        "    def __init__(self, data_path, intent_index, max_sequence_length, validation_split, only_last=False,\n",
        "                 random_state=None):\n",
        "        self.data_path = data_path\n",
        "        self.max_sequence_length = max_sequence_length\n",
        "        self.intent_index = intent_index\n",
        "        self.num_intents = max(intent_index.values()) + 1\n",
        "        self.only_last = only_last\n",
        "        self.file_list_train, self.file_list_test = self._split(validation_split, random_state=random_state)\n",
        "\n",
        "    def _file2sequence(self, file_and_path):\n",
        "        intent_list = []\n",
        "        embed_list = []\n",
        "        df1 = pd.read_hdf(file_and_path, engine=\"python\", encoding='cp1251')\n",
        "\n",
        "        for key, row in df1.iterrows():\n",
        "            intent = row['Intent analysis']\n",
        "            doc2vec = load_Doc2Vec_value(row[\"Doc2Vec value\"])\n",
        "            if intent:\n",
        "                intent_char = intent[0].lower()\n",
        "            else:\n",
        "                intent_char = \"\"\n",
        "            intent_list.append(self.intent_index[intent_char])\n",
        "            embed_list.append(doc2vec)\n",
        "        return intent_list, embed_list\n",
        "\n",
        "    def _split(self, validation_split, random_state=None):\n",
        "        file_list = os.listdir(self.data_path)\n",
        "        file_list_train, file_list_test = train_test_split(file_list, test_size=validation_split,\n",
        "                                                           random_state=random_state)\n",
        "        return file_list_train, file_list_test\n",
        "\n",
        "    def __build_intent_sequence(self, dialogs_list):\n",
        "        sequence_list = []\n",
        "        for dialog in dialogs_list:\n",
        "            sequence = []\n",
        "            for intent in dialog['Intent analysis'].values:\n",
        "                sequence.append(self.intent_index[intent])\n",
        "            sequence_list.append(sequence)\n",
        "        paded_sequences = pad_sequences(sequence_list, maxlen=self.max_sequence_length)\n",
        "        return paded_sequences\n",
        "\n",
        "    def generate_batch(self, batch_size, subset='training'):\n",
        "        if subset == 'training':\n",
        "            file_list = self.file_list_train\n",
        "        elif subset == 'validation':\n",
        "            file_list = self.file_list_test\n",
        "        f_i = 0\n",
        "\n",
        "        while True:\n",
        "            i = 0\n",
        "            sequence_batch, embed_batch = [], []\n",
        "            while i < batch_size:\n",
        "                if f_i == len(file_list):\n",
        "                    f_i = 0\n",
        "                    random.shuffle(file_list)\n",
        "                file_and_path = os.path.join(self.data_path, file_list[f_i])\n",
        "                intent_sequence, embed_list = self._file2sequence(file_and_path)\n",
        "                if len(intent_sequence) > self.max_sequence_length:\n",
        "                    for ii in range(len(intent_sequence) - self.max_sequence_length + 1):\n",
        "                        sequence_i = intent_sequence[ii:self.max_sequence_length + ii]\n",
        "                        sequence_batch.append(sequence_i)\n",
        "                        embed_list_i = embed_list[ii:self.max_sequence_length + ii]\n",
        "                        embed_batch.append(embed_list_i)\n",
        "                        i += 1\n",
        "                else:\n",
        "                    sequence_batch.append(intent_sequence)\n",
        "                    embed_batch.append(embed_list)\n",
        "                    # print(len(embed_list),len(embed_list[0]))\n",
        "                    i += 1\n",
        "                f_i += 1\n",
        "                # print(\"embed_list\",len(embed_list))\n",
        "                \n",
        "            paded_sequences = pad_sequences(sequence_batch, maxlen=self.max_sequence_length, dtype='float')\n",
        "            paded_embeds = pad_sequences(embed_batch, maxlen=self.max_sequence_length, dtype='float')\n",
        "            # print(paded_embeds)\n",
        "            inputs = paded_sequences[:, :-1]\n",
        "            if self.only_last:\n",
        "                labels = to_categorical(paded_sequences[:, -1:],\n",
        "                                        num_classes=self.num_intents)  # for build_CNN_model build_MLP_model\n",
        "            else:\n",
        "                labels = to_categorical(paded_sequences[:, 1:],\n",
        "                                        num_classes=self.num_intents)  # for build_BiRNN_model build_RNN_model\n",
        "            concat_inputs = np.concatenate((np.expand_dims(inputs,-1), np.array(paded_embeds[:, :-1])), 2)\n",
        "            yield concat_inputs, labels\n",
        "\n",
        "\n",
        "\n",
        "intents = {\"\": 0, \" \": 0, \"а\": 1, \"a\": 1, \"б\": 2, \"в\": 3, \"г\": 4, \"д\": 5,\n",
        "               \"е\": 6, \"e\": 6, \"ж\": 7, \"з\": 8, \"3\": 8, \"и\": 9, \"к\": 10,\n",
        "               \"л\": 11, \"м\": 12, \"н\": 13, \"о\": 14, \"п\": 15,\n",
        "               \"р\": 16, \"с\": 17, \"т\": 18, \"у\": 19, \"ф\": 20,\n",
        "               \"х\": 21, \"ц\": 22, \"ч\": 23, \"ш\": 24, \"щ\": 25}\n",
        "batch_size = 7\n",
        "max_sequence_length = 5\n",
        "validation_split = 0.2\n",
        "random_state = 42\n",
        "sg = SequenceGenerator(\"/content/feature_and_vector_seq\", intents, max_sequence_length, validation_split,\n",
        "                                    only_last=False,\n",
        "                                    random_state=random_state)\n",
        "inputs, labels = next(sg.generate_batch(batch_size, subset='training'))\n",
        "inputs.shape, labels.shape"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((11, 4, 301), (11, 4, 26))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3uevvu23qmS",
        "colab_type": "code",
        "outputId": "c60c0ff7-f503-4454-cb8a-b49652336fbe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras.layers import Dense, Input, GlobalMaxPooling1D, Flatten, TimeDistributed, Bidirectional\n",
        "from keras.layers import Conv1D, MaxPooling1D, Embedding, LSTM, Dropout, Activation, Permute, Lambda\n",
        "from keras import regularizers\n",
        "from keras.models import Model, Sequential\n",
        "from keras.callbacks import TensorBoard\n",
        "from keras.backend import permute_dimensions, squeeze, expand_dims\n",
        "from keras.layers.merge import concatenate\n",
        "# from dialog_intent_dl.ipynb.keras_handler.sequence_generator import SequenceGenerator\n",
        "# from dialog_intent_dl.ipynb.keras_handler.sequence_generator import PredictIntent\n",
        "print(tf.__version__)\n",
        "\n",
        "def freeze_session(session, keep_var_names=None, output_names=None, clear_devices=True):\n",
        "    \"\"\"\n",
        "    Freezes the state of a session into a pruned computation graph.\n",
        "    Creates a new computation graph where variable nodes are replaced by\n",
        "    constants taking their current value in the session. The new graph will be\n",
        "    pruned so subgraphs that are not necessary to compute the requested\n",
        "    outputs are removed.\n",
        "    @param session The TensorFlow session to be frozen.\n",
        "    @param keep_var_names A list of variable names that should not be frozen,\n",
        "                          or None to freeze all the variables in the graph.\n",
        "    @param output_names Names of the relevant graph outputs.\n",
        "    @param clear_devices Remove the device directives from the graph for better portability.\n",
        "    @return The frozen graph definition.\n",
        "    \"\"\"\n",
        "    graph = session.graph\n",
        "    with graph.as_default():\n",
        "        freeze_var_names = list(set(v.op.name for v in tf.global_variables()).difference(keep_var_names or []))\n",
        "        output_names = output_names or []\n",
        "        output_names += [v.op.name for v in tf.global_variables()]\n",
        "        input_graph_def = graph.as_graph_def()\n",
        "        if clear_devices:\n",
        "            for node in input_graph_def.node:\n",
        "                node.device = \"\"\n",
        "        frozen_graph = tf.graph_util.convert_variables_to_constants(\n",
        "            session, input_graph_def, output_names, freeze_var_names)\n",
        "        return frozen_graph\n",
        "    \n",
        "def save_model(session, model, name):\n",
        "    frozen_graph = freeze_session(session, output_names=[out.op.name for out in model.outputs])\n",
        "    tf.train.write_graph(frozen_graph, \"../models\", \"{}\".format(name), as_text=False)\n",
        "    \n",
        "class PredictIntent():\n",
        "    intents = {\"\": 0, \" \": 0, \"а\": 1, \"a\": 1, \"б\": 2, \"в\": 3, \"г\": 4, \"д\": 5,\n",
        "               \"е\": 6, \"e\": 6, \"ж\": 7, \"з\": 8, \"3\": 8, \"и\": 9, \"к\": 10,\n",
        "               \"л\": 11, \"м\": 12, \"н\": 13, \"о\": 14, \"п\": 15,\n",
        "               \"р\": 16, \"с\": 17, \"т\": 18, \"у\": 19, \"ф\": 20,\n",
        "               \"х\": 21, \"ц\": 22, \"ч\": 23, \"ш\": 24, \"щ\": 25}\n",
        "    general_intents = {\"\": 0, \" \": 0, \"а\": 1, \"a\": 1, \"б\": 1, \"в\": 1, \"г\": 1, \"д\": 1,  # Информативно-воспроизводящий\n",
        "                       \"е\": 2, \"e\": 2, \"ж\": 2, \"з\": 2, \"3\": 2, \"и\": 2, \"к\": 2,  # Эмотивно-консолидирующий\n",
        "                       \"л\": 3, \"м\": 3, \"н\": 3, \"о\": 3, \"п\": 3,  # Манипулятивный тип, доминирование\n",
        "                       \"р\": 4, \"с\": 4, \"т\": 4, \"у\": 4, \"ф\": 4,  # Волюнтивно-директивный\n",
        "                       \"х\": 5, \"ц\": 5, \"ч\": 5, \"ш\": 5, \"щ\": 5}  # Контрольно-реактивный\n",
        "    batch_size = 13\n",
        "    max_sequence_length = 5\n",
        "    intent_embedding_dim = 10\n",
        "    input_embedding_dim =300\n",
        "    num_units = 30\n",
        "    validation_split = 0.2\n",
        "    random_state = 42\n",
        "    tb = TensorBoard(log_dir='./logs', histogram_freq=0, batch_size=32, write_graph=True, write_grads=False,\n",
        "                     write_images=False, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None)\n",
        "\n",
        "    def __init__(self, is_general=False, data_path = \"../feature_and_vector_seq\"):\n",
        "        self.data_path = data_path\n",
        "        if is_general:\n",
        "            self.intent_index = self.general_intents\n",
        "        else:\n",
        "            self.intent_index = self.intents\n",
        "        self.num_intents = max(self.intent_index.values()) + 1\n",
        "        self.sg = SequenceGenerator(self.data_path, self.intent_index, self.max_sequence_length, self.validation_split,\n",
        "                                    only_last=False,\n",
        "                                    random_state=self.random_state)\n",
        "        print('num_intents', self.num_intents)\n",
        "\n",
        "\n",
        "    def build_MLP_model(self):\n",
        "        embedding_layer = Embedding(self.num_intents,\n",
        "                                    self.intent_embedding_dim,\n",
        "                                    input_length=self.max_sequence_length - 1,\n",
        "                                    trainable=True)\n",
        "        embedded_input = Input(shape=(self.max_sequence_length - 1, self.input_embedding_dim), dtype='float')\n",
        "        model = Sequential()\n",
        "        model.add(embedding_layer)\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(128, activation='relu', name=\"Dense1\"\n",
        "                        #                         , activity_regularizer=regularizers.l1(0.009)\n",
        "                        , kernel_regularizer=regularizers.l2(0.0001)\n",
        "                        #                         , bias_regularizer = regularizers.l2(0.009)\n",
        "                        ))  #\n",
        "        #         model.add(Dropout(0.2))\n",
        "        model.add(Dense(self.num_intents, activation='softmax', name=\"Dense2\"\n",
        "                        #                         , activity_regularizer=regularizers.l1(0.009)\n",
        "                        , kernel_regularizer=regularizers.l2(0.0001)\n",
        "                        #                         , bias_regularizer = regularizers.l2(0.009)\n",
        "                        ))\n",
        "        #         model.add(Dropout(0.2))\n",
        "        model.compile(loss='categorical_crossentropy',\n",
        "                      optimizer='rmsprop',\n",
        "                      metrics=['acc'])\n",
        "        self.model = model\n",
        "        self.sg.only_last = True\n",
        "        return self.model\n",
        "\n",
        "    def build_RNN_model(self):\n",
        "        embedding_layer = Embedding(self.num_intents,\n",
        "                                    self.intent_embedding_dim,\n",
        "                                    input_length=self.max_sequence_length - 1,\n",
        "                                    trainable=True)\n",
        "\n",
        "        sequence_input = Input(shape=(self.max_sequence_length - 1,), dtype='int32')\n",
        "        embedded_input = Input(shape=(self.max_sequence_length - 1, self.input_embedding_dim), dtype='float')\n",
        "        embedded_sequences = embedding_layer(sequence_input)\n",
        "\n",
        "        #         x_t = permute_dimensions(embedded_sequences, pattern=[0, 2, 1])\n",
        "        #         print('embedded_sequences',embedded_sequences.shape)\n",
        "        #         print('x_t',x_t.shape)\n",
        "\n",
        "        x = LSTM(self.num_units, dropout=0.2, recurrent_dropout=0.2, return_sequences=True)(embedded_sequences)\n",
        "        x = LSTM(self.num_units, dropout=0.2, recurrent_dropout=0.2, return_sequences=True)(x)\n",
        "        preds = TimeDistributed(Dense(self.num_intents, activation='softmax'))(x)\n",
        "        #         preds = Dense(self.num_intents, activation='softmax')(x)\n",
        "        #         print('preds.shape',preds.shape)\n",
        "\n",
        "        model = Model(sequence_input, embedded_input, preds)\n",
        "        model.compile(loss='categorical_crossentropy',\n",
        "                      optimizer='rmsprop',\n",
        "                      metrics=['acc'])\n",
        "        self.model = model\n",
        "        self.sg.only_last = False\n",
        "        return self.model\n",
        "\n",
        "    def build_BiRNN_model(self):\n",
        "        embedding_layer = Embedding(self.num_intents,\n",
        "                                    self.intent_embedding_dim,\n",
        "                                    input_length=self.max_sequence_length - 1,\n",
        "                                    trainable=True)\n",
        "        embedded_input = Input(shape=(self.max_sequence_length - 1, self.input_embedding_dim), dtype='float')\n",
        "        model = Sequential()\n",
        "        model.add(embedding_layer)\n",
        "        #         model.add(Permute([1, 2]))\n",
        "        model.add(Bidirectional(LSTM(self.num_units, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)))\n",
        "        model.add(Bidirectional(LSTM(self.num_units, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)))\n",
        "        #         model.add(Bidirectional(LSTM(self.num_units, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)))\n",
        "        model.add(\n",
        "            TimeDistributed(Dense(self.num_intents, activation='softmax', name=\"Dense2\"), name=\"TimeDistributed2\"))\n",
        "        model.compile(loss='categorical_crossentropy',\n",
        "                      optimizer='rmsprop',\n",
        "                      metrics=['acc'])\n",
        "        self.model = model\n",
        "        return self.model\n",
        "\n",
        "    def RNN_attention(self):\n",
        "        embedding_layer = Embedding(self.num_intents,\n",
        "                                    self.intent_embedding_dim,\n",
        "                                    input_length=self.max_sequence_length - 1,\n",
        "                                    trainable=True)\n",
        "        embedded_input = Input(shape=(self.max_sequence_length - 1, self.input_embedding_dim), dtype='float')\n",
        "        model1 = Sequential()\n",
        "        model1.add(embedding_layer)\n",
        "        model1.add(LSTM(self.num_units, return_sequences=True))\n",
        "\n",
        "        model2 = Sequential()\n",
        "        model2.add(Dense(input_dim=input_dim, output_dim=step))\n",
        "        model2.add(Activation('softmax'))  # Learn a probability distribution over each  step.\n",
        "        # Reshape to match LSTM's output shape, so that we can do element-wise multiplication.\n",
        "        model2.add(RepeatVector(self.num_units))\n",
        "        model2.add(Permute(2, 1))\n",
        "\n",
        "        model = Sequential()\n",
        "        model.add(\n",
        "            Merge([model1, model2], 'mul'))  # Multiply each element with corresponding weight a[i][j][k] * b[i][j]\n",
        "        model.add(TimeDistributedMergeinput_dim('sum'))  # Sum the weighted elements.\n",
        "\n",
        "        attention = Dense(1, activation='tanh')(activations)\n",
        "        attention = Flatten()(attention)\n",
        "        attention = Activation('softmax')(attention)\n",
        "        attention = RepeatVector(units)(attention)\n",
        "        attention = Permute([2, 1])(attention)\n",
        "\n",
        "        sent_representation = merge([activations, attention], mode='mul')\n",
        "        model.compile(loss='categorical_crossentropy',\n",
        "                      optimizer='rmsprop',\n",
        "                      metrics=['acc'])\n",
        "        self.model = model\n",
        "        self.sg = SequenceGenerator(self.data_path, self.intent_index, self.max_sequence_length, self.validation_split,\n",
        "                                    only_last=False)\n",
        "        return self.model\n",
        "\n",
        "    def build_CNN_model(self):\n",
        "        embedding_layer = Embedding(self.num_intents,\n",
        "                                    self.intent_embedding_dim,\n",
        "                                    input_length=self.max_sequence_length - 1,\n",
        "                                    trainable=True)\n",
        "        if self.text_embeddings:\n",
        "          concat_input = Input(shape=(self.max_sequence_length - 1,self.input_embedding_dim+1), dtype='float')\n",
        "          def get_part1(x):\n",
        "            split1, split2 = tf.split(x, [1, 300], 2)\n",
        "            return keras.backend.squeeze(tf.cast(split1, tf.int8), axis = 2)\n",
        "          def get_part2(x):\n",
        "            split1, split2 = tf.split(x, [1, 300], 2)\n",
        "            return split2\n",
        "\n",
        "          sequence_input = Lambda(get_part1)(concat_input)\n",
        "          embedded_input = Lambda(get_part2)(concat_input)\n",
        "          \n",
        "          embedded_sequences = embedding_layer(sequence_input)\n",
        "          out_embeddings = concatenate([embedded_input, embedded_sequences], axis = 2)\n",
        "          # print(\"concat_embeddings\",keras.backend.int_shape(concat_embeddings))\n",
        "        else:\n",
        "          sequence_input = Input(shape=(self.max_sequence_length - 1,), dtype='int32')\n",
        "          out_embeddings = embedding_layer(sequence_input)\n",
        "\n",
        "        x = Conv1D(128, 2, activation='relu', kernel_regularizer=regularizers.l2(0.0001))(out_embeddings)\n",
        "        \n",
        "        x = MaxPooling1D(1)(x)\n",
        "        x = Dropout(0.2)(x)\n",
        "        x = Conv1D(128, 3, activation='relu', kernel_regularizer=regularizers.l2(0.0001))(x)\n",
        "        x = GlobalMaxPooling1D()(x)\n",
        "        x = Dropout(0.2)(x)\n",
        "        x = Dense(128, activation='relu')(x)\n",
        "        preds = Dense(self.num_intents, activation='softmax')(x)\n",
        "\n",
        "        model = Model(concat_input, preds)\n",
        "        model.compile(loss='categorical_crossentropy',\n",
        "                      optimizer='rmsprop',\n",
        "                      metrics=['acc'])\n",
        "        self.model = model\n",
        "        self.sg.only_last = True\n",
        "        return self.model\n",
        "\n",
        "    def fit_generator(self, epochs):\n",
        "        history_nn = self.model.fit_generator(\n",
        "            generator=self.sg.generate_batch(self.batch_size, subset='training'),\n",
        "            steps_per_epoch=len(os.listdir(self.data_path)) * (1 - self.validation_split) // self.batch_size,\n",
        "            epochs=epochs,\n",
        "            validation_data=self.sg.generate_batch(self.batch_size, subset='validation'),\n",
        "            validation_steps=len(os.listdir(self.data_path)) * self.validation_split // self.batch_size + 1,\n",
        "            callbacks=[self.tb]\n",
        "        )\n",
        "        return history_nn"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.13.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ErhQ1GlJcBZp",
        "colab_type": "code",
        "outputId": "d9568230-d952-4e95-d385-9d70a97c929c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "PATH_TO_DATASET = \"/content/feature_and_vector_seq\"\n",
        "pi = PredictIntent(data_path = PATH_TO_DATASET, is_general = False)\n",
        "pi.batch_size = 7\n",
        "pi.max_sequence_length = 5\n",
        "pi.intent_embedding_dim = 10\n",
        "pi.num_units = 30\n",
        "pi.validation_split = 0.2\n",
        "pi.random_state = 42\n",
        "pi.text_embeddings = True"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "num_intents 26\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfBfvvwaccMb",
        "colab_type": "code",
        "outputId": "abe2cdce-d9df-4a95-f060-455dfc73299c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        }
      },
      "source": [
        "pi.build_CNN_model()\n",
        "history_CNN = pi.fit_generator(epochs = 10)\n",
        "from keras import backend as K\n",
        "save_model(K.get_session(), pi.model, 'history_CNN.pb')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "Epoch 1/10\n",
            "1501/1503 [============================>.] - ETA: 0s - loss: 2.8877 - acc: 0.1586"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEx-F_7NATcd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pi.RNN_attention()\n",
        "history_BiRNN = pi.fit_generator(epochs = 1)\n",
        "from keras import backend as K\n",
        "save_model(K.get_session(), pi.model, 'RNN_attention.pb')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUqWDZ4LwHAX",
        "colab_type": "code",
        "outputId": "964703ef-b7a2-45f1-c95c-eabd8262699c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 498
        }
      },
      "source": [
        "import pandas as pd\n",
        "import re, json\n",
        "\n",
        "def load_Doc2Vec_value(str1):\n",
        "  pattern1 = re.compile('\\n')\n",
        "  pattern2 = re.compile(' ')\n",
        "  str2 = pattern1.sub('', str1)\n",
        "  str3 = pattern2.sub(', ', str2)\n",
        "  return json.loads(str3)\n",
        "\n",
        "file_and_path = \"/content/feature_and_vector_seq/1-comm_inosmi_111_192344_output_10.h5\"\n",
        "df1 = pd.read_hdf(file_and_path, engine=\"python\", encoding='cp1251')\n",
        "df1[\"Doc2Vec\"] = df1[\"Doc2Vec value\"].apply(load_Doc2Vec_value)\n",
        "df1\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID of comment</th>\n",
              "      <th>ID of post</th>\n",
              "      <th>Likes</th>\n",
              "      <th>Intent analysis</th>\n",
              "      <th>Content analysis</th>\n",
              "      <th>Distance to parent</th>\n",
              "      <th>Distance to post</th>\n",
              "      <th>Doc2Vec value</th>\n",
              "      <th>Doc2Vec</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>192344</td>\n",
              "      <td>192344</td>\n",
              "      <td>0</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "      <td>1.1102230246251565e-16</td>\n",
              "      <td>[0.001903 0.008161 0.003022 0.021471 0.001911 ...</td>\n",
              "      <td>[0.001903, 0.008161, 0.003022, 0.021471, 0.001...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>192366</td>\n",
              "      <td>192344</td>\n",
              "      <td>0</td>\n",
              "      <td>м</td>\n",
              "      <td>1</td>\n",
              "      <td>0.9047975317843413</td>\n",
              "      <td>0.9047975317843413</td>\n",
              "      <td>[0.015871 0.063059 0.027442 0.005918 0.067336 ...</td>\n",
              "      <td>[0.015871, 0.063059, 0.027442, 0.005918, 0.067...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>192369</td>\n",
              "      <td>192344</td>\n",
              "      <td>1</td>\n",
              "      <td>а</td>\n",
              "      <td>3</td>\n",
              "      <td>0.9263160364648992</td>\n",
              "      <td>0.949031225401245</td>\n",
              "      <td>[-0.050479 -0.020054 0.010354 0.009482 -0.0072...</td>\n",
              "      <td>[-0.050479, -0.020054, 0.010354, 0.009482, -0....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>192370</td>\n",
              "      <td>192344</td>\n",
              "      <td>0</td>\n",
              "      <td>ч</td>\n",
              "      <td>3</td>\n",
              "      <td>0.8536771566133271</td>\n",
              "      <td>1.0161022628113872</td>\n",
              "      <td>[-0.003400 0.004530 -0.011841 0.009115 0.01707...</td>\n",
              "      <td>[-0.0034, 0.00453, -0.011841, 0.009115, 0.0170...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>192371</td>\n",
              "      <td>192344</td>\n",
              "      <td>1</td>\n",
              "      <td>н</td>\n",
              "      <td>4</td>\n",
              "      <td>0.4652375928796576</td>\n",
              "      <td>1.1818890054062847</td>\n",
              "      <td>[-0.017641 0.020937 -0.043087 0.017828 0.03717...</td>\n",
              "      <td>[-0.017641, 0.020937, -0.043087, 0.017828, 0.0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>192372</td>\n",
              "      <td>192344</td>\n",
              "      <td>0</td>\n",
              "      <td>р</td>\n",
              "      <td>4</td>\n",
              "      <td>0.5322816918635409</td>\n",
              "      <td>0.918104637002647</td>\n",
              "      <td>[0.014156 0.003831 0.001797 0.009888 0.004583 ...</td>\n",
              "      <td>[0.014156, 0.003831, 0.001797, 0.009888, 0.004...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>192377</td>\n",
              "      <td>192344</td>\n",
              "      <td>0</td>\n",
              "      <td>ф</td>\n",
              "      <td>3</td>\n",
              "      <td>0.9230922955341927</td>\n",
              "      <td>0.5331857866510279</td>\n",
              "      <td>[-0.026679 -0.030724 0.026273 0.014249 0.00759...</td>\n",
              "      <td>[-0.026679, -0.030724, 0.026273, 0.014249, 0.0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>192381</td>\n",
              "      <td>192344</td>\n",
              "      <td>0</td>\n",
              "      <td>ч</td>\n",
              "      <td>4</td>\n",
              "      <td>0.8990398151377282</td>\n",
              "      <td>0.9101620933407938</td>\n",
              "      <td>[0.008495 0.029132 -0.002679 -0.007744 0.01133...</td>\n",
              "      <td>[0.008495, 0.029132, -0.002679, -0.007744, 0.0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>192395</td>\n",
              "      <td>192344</td>\n",
              "      <td>1</td>\n",
              "      <td>ч</td>\n",
              "      <td>3</td>\n",
              "      <td>1.0171192821636792</td>\n",
              "      <td>0.7188240607558654</td>\n",
              "      <td>[-0.009065 -0.010122 0.035569 -0.002556 -0.003...</td>\n",
              "      <td>[-0.009065, -0.010122, 0.035569, -0.002556, -0...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  ...\n",
              "0 ...\n",
              "1 ...\n",
              "2 ...\n",
              "3 ...\n",
              "4 ...\n",
              "5 ...\n",
              "6 ...\n",
              "7 ...\n",
              "8 ...\n",
              "\n",
              "[9 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85_Hp2tYcj9I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_U4opvW6xx4N",
        "colab_type": "code",
        "outputId": "62d61702-1a5a-43de-bbc3-3c04220a1fd9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "data = np.random.rand(200,2)\n",
        "expected = np.random.randint(2, size=200).reshape(-1,1)\n",
        "\n",
        "dataFrame = pd.DataFrame(data, columns = ['a','b'])\n",
        "expectedFrame = pd.DataFrame(expected, columns = ['expected'])\n",
        "\n",
        "dataFrameTrain, dataFrameTest = dataFrame[:100],dataFrame[-100:]\n",
        "expectedFrameTrain, expectedFrameTest = expectedFrame[:100],expectedFrame[-100:]\n",
        "\n",
        "def generator(X_data, y_data, batch_size):\n",
        "\n",
        "  samples_per_epoch = X_data.shape[0]\n",
        "  number_of_batches = samples_per_epoch/batch_size\n",
        "  counter=0\n",
        "  while 1:\n",
        "    X_batch = np.array(X_data[batch_size*counter:batch_size*(counter+1)]).astype('float32')\n",
        "    y_batch = np.array(y_data[batch_size*counter:batch_size*(counter+1)]).astype('float32')\n",
        "    counter += 1\n",
        "    yield X_batch,y_batch\n",
        "\n",
        "    #restart counter to yeild data in the next epoch as well\n",
        "    if counter >= number_of_batches:\n",
        "        counter = 0\n",
        "\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Dropout, Activation, Flatten, Reshape\n",
        "from keras.layers.convolutional import Convolution1D, Convolution2D, MaxPooling2D\n",
        "from keras.utils import np_utils\n",
        "\n",
        "inputs1 = Input(shape=(dataFrame.shape[1],))\n",
        "# inputs2 = Input(shape=(dataFrame.shape[1],))\n",
        "x = Dense(12, activation='relu', input_dim=dataFrame.shape[1])(inputs1)\n",
        "preds = Dense(1, activation='sigmoid')(x)\n",
        "model = Model(inputs1, preds)\n",
        "# model = Sequential()\n",
        "# model.add(Dense(12, activation='relu', input_dim=dataFrame.shape[1]))\n",
        "# model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adadelta', metrics=['accuracy'])\n",
        "\n",
        "#Train the model using generator vs using the full batch\n",
        "batch_size = 8\n",
        "\n",
        "model.fit_generator(generator(dataFrameTrain,expectedFrameTrain,batch_size), epochs=3,steps_per_epoch = dataFrame.shape[0]/batch_size, validation_data=generator(dataFrameTest,expectedFrameTest,batch_size*2),validation_steps=dataFrame.shape[0]/batch_size*2)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "25/25 [==============================] - 2s 68ms/step - loss: 0.6932 - acc: 0.5346 - val_loss: 0.6949 - val_acc: 0.4735\n",
            "Epoch 2/3\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.6926 - acc: 0.5242 - val_loss: 0.6950 - val_acc: 0.4832\n",
            "Epoch 3/3\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.6915 - acc: 0.5493 - val_loss: 0.6952 - val_acc: 0.4707\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f1364c31128>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZhjTP-mhzkwD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}