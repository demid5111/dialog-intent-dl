{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PredictIntent.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/demid5111/dialog-intent-dl/blob/master/ipynb/PredictOnColab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tE1UJlzMur_k",
        "colab_type": "code",
        "outputId": "707c868b-e110-482d-9d21-a57c4c56aa79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anjRwJ1o0yqo",
        "colab_type": "code",
        "outputId": "2dcf7b3e-ce62-49e7-8440-1d5c822e9f19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "! tar -xzf \"/content/drive/My Drive/Projects/RFFI2016/dataset.tar.gz\" -C /content\n",
        "! ls \"/content/feature_and_vector_seq\" | wc -l"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "13156\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWDAX-g82iqa",
        "colab_type": "code",
        "outputId": "144c9380-3d9c-4a60-bb84-395f4a69e4e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 344
        }
      },
      "source": [
        "! git clone https://github.com/demid5111/dialog-intent-dl.git\n",
        "! pip install -r dialog-intent-dl/requirements.txt\n",
        "! pip install tensorflow-gpu==1.13.1\n",
        "! pip install numpy==1.15.4"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting numpy==1.15.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ff/7f/9d804d2348471c67a7d8b5f84f9bc59fd1cefa148986f2b74552f8573555/numpy-1.15.4-cp36-cp36m-manylinux1_x86_64.whl (13.9MB)\n",
            "\u001b[K     |████████████████████████████████| 13.9MB 34.2MB/s \n",
            "\u001b[31mERROR: tensorflow 1.14.0rc1 has requirement tensorflow-estimator<1.15.0rc0,>=1.14.0rc0, but you'll have tensorflow-estimator 1.13.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement pandas~=0.24.0, but you'll have pandas 0.23.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: fbprophet 0.5 has requirement pandas>=0.23.4, but you'll have pandas 0.23.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Found existing installation: numpy 1.14.5\n",
            "    Uninstalling numpy-1.14.5:\n",
            "      Successfully uninstalled numpy-1.14.5\n",
            "Successfully installed numpy-1.15.4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpM_A-PW3VMn",
        "colab_type": "code",
        "outputId": "ef9d79b9-9c84-470c-b23b-926938386905",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "! ls "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dialog_intent_dl  drive  feature_and_vector_seq  logs  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3uevvu23qmS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1a30d0eb-caa7-4865-fe01-40f96f145c6c"
      },
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras.layers import Dense, Input, GlobalMaxPooling1D, Flatten, TimeDistributed, Bidirectional\n",
        "from keras.layers import Conv1D, MaxPooling1D, Embedding, LSTM, Dropout, Activation, Permute\n",
        "from keras import regularizers\n",
        "from keras.models import Model, Sequential\n",
        "from keras.callbacks import TensorBoard\n",
        "from keras.backend import permute_dimensions\n",
        "from dialog_intent_dl.ipynb.keras_handler.sequence_generator import SequenceGenerator\n",
        "print(tf.__version__)\n",
        "\n",
        "class PredictIntent():\n",
        "    intents = {\"\": 0, \" \": 0, \"а\": 1, \"a\": 1, \"б\": 2, \"в\": 3, \"г\": 4, \"д\": 5,\n",
        "               \"е\": 6, \"e\": 6, \"ж\": 7, \"з\": 8, \"3\": 8, \"и\": 9, \"к\": 10,\n",
        "               \"л\": 11, \"м\": 12, \"н\": 13, \"о\": 14, \"п\": 15,\n",
        "               \"р\": 16, \"с\": 17, \"т\": 18, \"у\": 19, \"ф\": 20,\n",
        "               \"х\": 21, \"ц\": 22, \"ч\": 23, \"ш\": 24, \"щ\": 25}\n",
        "    general_intents = {\"\": 0, \" \": 0, \"а\": 1, \"a\": 1, \"б\": 1, \"в\": 1, \"г\": 1, \"д\": 1,  # Информативно-воспроизводящий\n",
        "                       \"е\": 2, \"e\": 2, \"ж\": 2, \"з\": 2, \"3\": 2, \"и\": 2, \"к\": 2,  # Эмотивно-консолидирующий\n",
        "                       \"л\": 3, \"м\": 3, \"н\": 3, \"о\": 3, \"п\": 3,  # Манипулятивный тип, доминирование\n",
        "                       \"р\": 4, \"с\": 4, \"т\": 4, \"у\": 4, \"ф\": 4,  # Волюнтивно-директивный\n",
        "                       \"х\": 5, \"ц\": 5, \"ч\": 5, \"ш\": 5, \"щ\": 5}  # Контрольно-реактивный\n",
        "    batch_size = 13\n",
        "    max_sequence_length = 5\n",
        "    intent_embedding_dim = 10\n",
        "    num_units = 30\n",
        "    validation_split = 0.2\n",
        "    random_state = 42\n",
        "    tb = TensorBoard(log_dir='./logs', histogram_freq=0, batch_size=32, write_graph=True, write_grads=False,\n",
        "                     write_images=False, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None)\n",
        "\n",
        "    def __init__(self, is_general=False, data_path = \"../feature_and_vector_seq\"):\n",
        "        self.data_path = data_path\n",
        "        if is_general:\n",
        "            self.intent_index = self.general_intents\n",
        "        else:\n",
        "            self.intent_index = self.intents\n",
        "        self.num_intents = max(self.intent_index.values()) + 1\n",
        "        self.sg = SequenceGenerator(self.data_path, self.intent_index, self.max_sequence_length, self.validation_split,\n",
        "                                    only_last=False,\n",
        "                                    random_state=self.random_state)\n",
        "        print('num_intents', self.num_intents)\n",
        "\n",
        "    def build_CNN_model(self):\n",
        "        embedding_layer = Embedding(self.num_intents,\n",
        "                                    self.intent_embedding_dim,\n",
        "                                    input_length=self.max_sequence_length - 1,\n",
        "                                    trainable=True)\n",
        "\n",
        "        sequence_input = Input(shape=(self.max_sequence_length - 1,), dtype='int32')\n",
        "        embedded_sequences = embedding_layer(sequence_input)\n",
        "        #         x_p = permute_dimensions(embedded_sequences, pattern=[0, 2, 1])\n",
        "\n",
        "        x = Conv1D(128, 2, activation='relu', kernel_regularizer=regularizers.l2(0.0001))(embedded_sequences)\n",
        "        x = MaxPooling1D(1)(x)\n",
        "        x = Dropout(0.2)(x)\n",
        "        x = Conv1D(128, 3, activation='relu', kernel_regularizer=regularizers.l2(0.0001))(x)\n",
        "        x = GlobalMaxPooling1D()(x)\n",
        "        x = Dropout(0.2)(x)\n",
        "        x = Dense(128, activation='relu')(x)\n",
        "        preds = Dense(self.num_intents, activation='softmax')(x)\n",
        "\n",
        "        model = Model(sequence_input, preds)\n",
        "        model.compile(loss='categorical_crossentropy',\n",
        "                      optimizer='rmsprop',\n",
        "                      metrics=['acc'])\n",
        "        self.model = model\n",
        "        self.sg.only_last = True\n",
        "        return self.model\n",
        "\n",
        "    def build_MLP_model(self):\n",
        "        embedding_layer = Embedding(self.num_intents,\n",
        "                                    self.intent_embedding_dim,\n",
        "                                    input_length=self.max_sequence_length - 1,\n",
        "                                    trainable=True)\n",
        "\n",
        "        model = Sequential()\n",
        "        model.add(embedding_layer)\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(128, activation='relu', name=\"Dense1\"\n",
        "                        #                         , activity_regularizer=regularizers.l1(0.009)\n",
        "                        , kernel_regularizer=regularizers.l2(0.0001)\n",
        "                        #                         , bias_regularizer = regularizers.l2(0.009)\n",
        "                        ))  #\n",
        "        #         model.add(Dropout(0.2))\n",
        "        model.add(Dense(self.num_intents, activation='softmax', name=\"Dense2\"\n",
        "                        #                         , activity_regularizer=regularizers.l1(0.009)\n",
        "                        , kernel_regularizer=regularizers.l2(0.0001)\n",
        "                        #                         , bias_regularizer = regularizers.l2(0.009)\n",
        "                        ))\n",
        "        #         model.add(Dropout(0.2))\n",
        "        model.compile(loss='categorical_crossentropy',\n",
        "                      optimizer='rmsprop',\n",
        "                      metrics=['acc'])\n",
        "        self.model = model\n",
        "        self.sg.only_last = True\n",
        "        return self.model\n",
        "\n",
        "    def build_RNN_model(self):\n",
        "        embedding_layer = Embedding(self.num_intents,\n",
        "                                    self.intent_embedding_dim,\n",
        "                                    input_length=self.max_sequence_length - 1,\n",
        "                                    trainable=True)\n",
        "\n",
        "        sequence_input = Input(shape=(self.max_sequence_length - 1,), dtype='int32')\n",
        "        embedded_sequences = embedding_layer(sequence_input)\n",
        "\n",
        "        #         x_t = permute_dimensions(embedded_sequences, pattern=[0, 2, 1])\n",
        "        #         print('embedded_sequences',embedded_sequences.shape)\n",
        "        #         print('x_t',x_t.shape)\n",
        "\n",
        "        x = LSTM(self.num_units, dropout=0.2, recurrent_dropout=0.2, return_sequences=True)(embedded_sequences)\n",
        "        x = LSTM(self.num_units, dropout=0.2, recurrent_dropout=0.2, return_sequences=True)(x)\n",
        "        preds = TimeDistributed(Dense(self.num_intents, activation='softmax'))(x)\n",
        "        #         preds = Dense(self.num_intents, activation='softmax')(x)\n",
        "        #         print('preds.shape',preds.shape)\n",
        "\n",
        "        model = Model(sequence_input, preds)\n",
        "        model.compile(loss='categorical_crossentropy',\n",
        "                      optimizer='rmsprop',\n",
        "                      metrics=['acc'])\n",
        "        self.model = model\n",
        "        self.sg.only_last = False\n",
        "        return self.model\n",
        "\n",
        "    def build_BiRNN_model(self):\n",
        "        embedding_layer = Embedding(self.num_intents,\n",
        "                                    self.intent_embedding_dim,\n",
        "                                    input_length=self.max_sequence_length - 1,\n",
        "                                    trainable=True)\n",
        "        model = Sequential()\n",
        "        model.add(embedding_layer)\n",
        "        #         model.add(Permute([1, 2]))\n",
        "        model.add(Bidirectional(LSTM(self.num_units, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)))\n",
        "        model.add(Bidirectional(LSTM(self.num_units, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)))\n",
        "        #         model.add(Bidirectional(LSTM(self.num_units, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)))\n",
        "        model.add(\n",
        "            TimeDistributed(Dense(self.num_intents, activation='softmax', name=\"Dense2\"), name=\"TimeDistributed2\"))\n",
        "        model.compile(loss='categorical_crossentropy',\n",
        "                      optimizer='rmsprop',\n",
        "                      metrics=['acc'])\n",
        "        self.model = model\n",
        "        return self.model\n",
        "\n",
        "    def RNN_attention():\n",
        "\n",
        "        embedding_layer = Embedding(self.num_intents,\n",
        "                                    self.intent_embedding_dim,\n",
        "                                    input_length=self.max_sequence_length - 1,\n",
        "                                    trainable=True)\n",
        "        model1 = Sequential()\n",
        "        model1.add(embedding_layer)\n",
        "        model1.add(LSTM(self.num_units, return_sequences=True))\n",
        "\n",
        "        model2 = Sequential()\n",
        "        model2.add(Dense(input_dim=input_dim, output_dim=step))\n",
        "        model2.add(Activation('softmax'))  # Learn a probability distribution over each  step.\n",
        "        # Reshape to match LSTM's output shape, so that we can do element-wise multiplication.\n",
        "        model2.add(RepeatVector(self.num_units))\n",
        "        model2.add(Permute(2, 1))\n",
        "\n",
        "        model = Sequential()\n",
        "        model.add(\n",
        "            Merge([model1, model2], 'mul'))  # Multiply each element with corresponding weight a[i][j][k] * b[i][j]\n",
        "        model.add(TimeDistributedMerge('sum'))  # Sum the weighted elements.\n",
        "\n",
        "        attention = Dense(1, activation='tanh')(activations)\n",
        "        attention = Flatten()(attention)\n",
        "        attention = Activation('softmax')(attention)\n",
        "        attention = RepeatVector(units)(attention)\n",
        "        attention = Permute([2, 1])(attention)\n",
        "\n",
        "        sent_representation = merge([activations, attention], mode='mul')\n",
        "        model.compile(loss='categorical_crossentropy',\n",
        "                      optimizer='rmsprop',\n",
        "                      metrics=['acc'])\n",
        "        self.model = model\n",
        "        self.sg = SequenceGenerator(self.data_path, self.intent_index, self.max_sequence_length, self.validation_split,\n",
        "                                    only_last=False)\n",
        "        return self.model\n",
        "\n",
        "    def fit_generator(self, epochs):\n",
        "        history_nn = self.model.fit_generator(\n",
        "            generator=self.sg.generate_batch(self.batch_size, subset='training'),\n",
        "            steps_per_epoch=len(os.listdir(self.data_path)) * (1 - self.validation_split) // self.batch_size,\n",
        "            epochs=epochs,\n",
        "            validation_data=self.sg.generate_batch(self.batch_size, subset='validation'),\n",
        "            validation_steps=len(os.listdir(self.data_path)) * self.validation_split // self.batch_size + 1,\n",
        "            callbacks=[self.tb]\n",
        "        )\n",
        "        return history_nn\n",
        "      \n",
        "def freeze_session(session, keep_var_names=None, output_names=None, clear_devices=True):\n",
        "    \"\"\"\n",
        "    Freezes the state of a session into a pruned computation graph.\n",
        "    Creates a new computation graph where variable nodes are replaced by\n",
        "    constants taking their current value in the session. The new graph will be\n",
        "    pruned so subgraphs that are not necessary to compute the requested\n",
        "    outputs are removed.\n",
        "    @param session The TensorFlow session to be frozen.\n",
        "    @param keep_var_names A list of variable names that should not be frozen,\n",
        "                          or None to freeze all the variables in the graph.\n",
        "    @param output_names Names of the relevant graph outputs.\n",
        "    @param clear_devices Remove the device directives from the graph for better portability.\n",
        "    @return The frozen graph definition.\n",
        "    \"\"\"\n",
        "    graph = session.graph\n",
        "    with graph.as_default():\n",
        "        freeze_var_names = list(set(v.op.name for v in tf.global_variables()).difference(keep_var_names or []))\n",
        "        output_names = output_names or []\n",
        "        output_names += [v.op.name for v in tf.global_variables()]\n",
        "        input_graph_def = graph.as_graph_def()\n",
        "        if clear_devices:\n",
        "            for node in input_graph_def.node:\n",
        "                node.device = \"\"\n",
        "        frozen_graph = tf.graph_util.convert_variables_to_constants(\n",
        "            session, input_graph_def, output_names, freeze_var_names)\n",
        "        return frozen_graph\n",
        "    \n",
        "def save_model(session, model, name):\n",
        "    frozen_graph = freeze_session(session, output_names=[out.op.name for out in model.outputs])\n",
        "    tf.train.write_graph(frozen_graph, \"../models\", \"{}\".format(name), as_text=False)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.13.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ErhQ1GlJcBZp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1b95c21f-7d0c-4bab-c769-473be2e16f43"
      },
      "source": [
        "PATH_TO_DATASET = \"/content/feature_and_vector_seq\"\n",
        "pi = PredictIntent(data_path = PATH_TO_DATASET, is_general = False)\n",
        "pi.batch_size = 7\n",
        "pi.max_sequence_length = 5\n",
        "pi.intent_embedding_dim = 10\n",
        "pi.num_units = 30\n",
        "pi.validation_split = 0.2\n",
        "pi.random_state = 42"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "num_intents 26\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfBfvvwaccMb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pi.build_BiRNN_model()\n",
        "history_BiRNN = pi.fit_generator(epochs = 1)\n",
        "from keras import backend as K\n",
        "save_model(K.get_session(), history_BiRNN, 'history_BiRNN.pb')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEx-F_7NATcd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUqWDZ4LwHAX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "6a61384d-d3d2-4848-fe24-0314093191ca"
      },
      "source": [
        "import pandas as pd\n",
        "file_and_path = \"/content/feature_and_vector_seq/1-comm_inosmi_111_192344_output_10.h5\"\n",
        "df1 = pd.read_hdf(file_and_path, engine=\"python\", encoding='cp1251')\n",
        "df1[\"Doc2Vec value\"][0]#.astype(float)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'[0.001903 0.008161 0.003022 0.021471 0.001911 0.000769 0.027428 -0.027831\\n -0.003073 -0.004980 0.020644 -0.014960 0.008604 0.012825 -0.026068\\n 0.020403 0.006612 0.025695 0.017430 -0.047408 -0.015395 -0.019598\\n 0.027972 -0.000601 0.030375 0.011772 0.021578 -0.013088 -0.006355\\n -0.009098 -0.004229 0.010379 0.017166 -0.042873 0.018200 -0.000433\\n -0.023537 0.001672 -0.032163 0.007928 0.057112 0.003129 0.006764 0.001141\\n -0.016615 -0.019479 -0.036450 0.000785 -0.017258 -0.014166 0.014459\\n -0.022591 -0.007363 -0.006674 -0.009149 -0.031017 0.016442 0.002262\\n -0.003950 0.027914 0.034671 0.019261 -0.000262 0.015857 -0.005982\\n 0.017371 -0.013526 -0.014391 0.010246 0.009417 -0.006874 -0.031878\\n -0.000649 0.000668 0.012658 0.010711 0.020093 -0.002385 0.014615\\n -0.005328 0.004366 -0.029982 0.015387 -0.040169 -0.014782 -0.006836\\n -0.038126 -0.000182 0.050316 -0.006597 -0.003478 -0.019885 -0.012636\\n -0.019870 -0.016595 -0.006353 -0.031023 -0.007265 0.009909 -0.010244\\n -0.010868 -0.012722 -0.012167 0.007303 -0.008510 0.020125 0.008743\\n 0.028610 -0.023170 -0.007105 -0.017366 -0.030124 -0.003691 -0.033025\\n 0.003141 0.015386 0.026727 -0.016831 0.015945 -0.004011 0.008882\\n -0.007397 0.009991 -0.019593 0.028182 -0.016615 0.017724 0.020502\\n -0.004743 0.020859 -0.017111 0.004987 -0.028042 0.037981 0.016564\\n 0.032276 -0.009798 -0.006056 -0.005069 0.028353 0.011338 -0.018771\\n -0.028280 -0.001765 0.036952 -0.001800 -0.000669 0.002354 -0.003807\\n -0.046471 0.006740 0.027786 -0.033839 0.046181 0.016976 -0.018619\\n 0.030660 -0.015722 0.023308 -0.012504 -0.012198 0.005808 0.011137\\n -0.013401 -0.015512 0.009721 0.006436 -0.016104 0.033862 0.019661\\n -0.035553 -0.026034 0.013050 -0.000155 0.003462 0.014282 -0.062348\\n 0.003524 0.003156 0.010944 -0.027759 -0.029644 0.003830 -0.006201\\n -0.029838 0.018758 -0.002161 0.013143 0.024630 0.012040 -0.006549\\n 0.000616 0.029546 0.024754 -0.003767 -0.018399 0.015970 0.006237 0.006982\\n 0.014317 0.015592 -0.019014 -0.014806 0.023027 0.011065 0.006440 0.006627\\n -0.005942 0.007627 -0.052866 -0.000914 -0.012337 -0.009080 -0.015774\\n -0.012697 -0.002863 -0.024574 -0.007252 -0.002168 0.029740 0.010841\\n -0.002534 0.013982 0.044423 0.007351 0.016465 -0.027703 0.036442\\n -0.019818 -0.003133 -0.012234 -0.051162 -0.021196 -0.032928 -0.001384\\n 0.004951 0.004035 -0.029732 0.017077 0.005049 -0.027153 0.016827 0.018750\\n 0.013800 0.023262 -0.008851 0.006244 -0.025100 0.017823 -0.003061\\n -0.035579 0.020160 -0.002895 -0.010712 0.010940 0.014147 -0.012910\\n 0.021447 0.024959 0.023514 0.039229 -0.033398 0.010518 0.023849 0.046207\\n 0.018754 0.013152 -0.019885 0.027359 -0.028596 0.018588 -0.008172\\n 0.007063 0.010495 0.014247 -0.011368 0.015759 0.025840 0.012232 -0.014273\\n -0.017942 -0.008590 0.012736 0.022111 -0.030675 0.004306 0.002245\\n -0.002020 -0.001518 0.014225 0.030277 -0.039498 0.005450 0.041083\\n 0.006064 -0.020400 0.010224 0.006117 -0.027193 0.011287]'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85_Hp2tYcj9I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_U4opvW6xx4N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}